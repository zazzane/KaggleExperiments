{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 3 Fine-Tuning on SQuAD Dataset\n\nThis notebook demonstrates how to fine-tune the Gemma 3 4B model on the Stanford Question Answering Dataset (SQuAD).","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q -U transformers datasets peft bitsandbytes accelerate trl immutabledict sentencepiece wandb\n!git clone https://github.com/google/gemma_pytorch.git","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Disable W&B logging for this run\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport kagglehub\nimport sys\nimport contextlib\nfrom datasets import load_dataset\nimport transformers\nfrom transformers import AutoTokenizer\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom trl import SFTTrainer\n\n# Add gemma_pytorch to path\nsys.path.append(\"/kaggle/working/gemma_pytorch/\")\n\nfrom gemma.config import get_model_config\nfrom gemma.gemma3_model import Gemma3ForMultimodalLM\n\n# Check CUDA capabilities and set device\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU count: {torch.cuda.device_count()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU capabilities: {torch.cuda.get_device_capability(0)}\")\n    \n# Set a single GPU to use (to avoid multi-GPU issues)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load Gemma 3 Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Model configuration\nVARIANT = '4b'\nMETHOD = 'it'  # instruction-tuned model\n\n# Determine if we can use GPU and what precision to use\nif torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n    MACHINE_TYPE = 'cuda'\n    torch_dtype = torch.bfloat16\nelif torch.cuda.is_available():\n    MACHINE_TYPE = 'cuda'\n    torch_dtype = torch.float16\nelse:\n    MACHINE_TYPE = 'cpu'\n    torch_dtype = torch.float32\n    \nprint(f\"Using {MACHINE_TYPE} with {torch_dtype}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download model weights\nweights_dir = kagglehub.model_download(f\"google/gemma-3/pytorch/gemma-3-{VARIANT}-{METHOD}/1\")\ntokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\nckpt_path = os.path.join(weights_dir, f'model.ckpt')\n\n# Set up model config\nmodel_config = get_model_config(VARIANT)\nmodel_config.dtype = \"float32\" if MACHINE_TYPE == \"cpu\" else \"float16\"\nmodel_config.tokenizer = tokenizer_path\n\n# Helper function to set default tensor type\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model in 4-bit precision with LORA adapters for efficient fine-tuning\ndevice = torch.device(MACHINE_TYPE)\n\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = Gemma3ForMultimodalLM(model_config)\n    model.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n    \n    # Initialize tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Prepare Model for Fine-Tuning with PEFT","metadata":{}},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA configuration\n# Note: Target modules may need adjustment for Gemma 3 architecture\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\n# Apply PEFT configuration to model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Load and Prepare SQuAD Dataset","metadata":{}},{"cell_type":"code","source":"# Load SQuAD dataset\nsquad_dataset = load_dataset(\"squad\")\nprint(squad_dataset)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Format SQuAD examples for instruction fine-tuning\n# We'll use a specific format tailored for Gemma 3's chat template\n\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\nContext: {context}\\n\\nQuestion: {question}<end_of_turn>\\n\"\nMODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{answer}<end_of_turn>\\n\"\n\ndef format_squad_example(example):\n    user_prompt = USER_CHAT_TEMPLATE.format(\n        context=example[\"context\"],\n        question=example[\"question\"]\n    )\n    model_response = MODEL_CHAT_TEMPLATE.format(answer=example[\"answers\"][\"text\"][0])\n    return {\n        \"formatted_prompt\": user_prompt + model_response,\n        \"input\": user_prompt,\n        \"output\": model_response\n    }\n\n# Apply formatting to the dataset\ntrain_dataset = squad_dataset[\"train\"].map(format_squad_example)\nvalidation_dataset = squad_dataset[\"validation\"].map(format_squad_example)\n\n# Take a subset for faster experimentation\ntrain_subset = train_dataset.select(range(1000))  # Adjust as needed\nvalidation_subset = validation_dataset.select(range(100))  # Adjust as needed\n\nprint(f\"Training examples: {len(train_subset)}\")\nprint(f\"Validation examples: {len(validation_subset)}\")\n\n# Display an example\nprint(\"\\nExample of formatted data:\")\nprint(train_subset[0][\"formatted_prompt\"])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Configure Training","metadata":{}},{"cell_type":"code","source":"# Set training arguments\ntraining_args = transformers.TrainingArguments(\n    output_dir=\"./gemma3_squad_results\",\n    eval_strategy=\"steps\",\n    evaluation_strategy=\"steps\",  # More explicit parameter name\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    warmup_steps=2,\n    max_steps=100,  # Adjust based on available time/resources\n    learning_rate=2e-5,  # Slightly lower than with Gemma 2\n    fp16=True if torch_dtype == torch.float16 else False,\n    bf16=True if torch_dtype == torch.bfloat16 else False,\n    optim=\"paged_adamw_8bit\",\n    save_strategy=\"steps\",\n    save_steps=50,\n    eval_steps=25,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    push_to_hub=False,\n    report_to=\"none\",  # Disable reporting to wandb\n    run_name=\"gemma3-squad-finetune\"\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_subset,\n    eval_dataset=validation_subset,\n    args=training_args,\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_prompt\",\n    max_seq_length=512,  # Adjust based on your context length needs\n    tokenizer=tokenizer,\n    packing=False  # Set to False to avoid truncating examples\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Train the Model","metadata":{}},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel_save_path = \"./gemma3_squad_finetuned\"\ntrainer.model.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Inference with the Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"# Load the fine-tuned model for inference\nfrom peft import PeftModel, PeftConfig\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(model_save_path)\n\n# Reload model with the fine-tuned weights\nwith _set_default_tensor_type(model_config.get_dtype()):\n    eval_model = Gemma3ForMultimodalLM(model_config)\n    eval_model.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n    \n# Load the PEFT model\neval_model = PeftModel.from_pretrained(eval_model, model_save_path)\neval_model = eval_model.to(device).eval()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for question answering with the fine-tuned model\ndef answer_question(context, question, output_len=50):\n    user_prompt = USER_CHAT_TEMPLATE.format(context=context, question=question)\n    \n    # Tokenize input\n    inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate answer\n    with torch.no_grad():\n        outputs = eval_model.generate(\n            inputs.input_ids,\n            max_new_tokens=output_len,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    # Decode the generated text and extract the answer\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract the model's answer part\n    model_part = generated_text[len(user_prompt):]\n    \n    # Remove the model chat template if present\n    if \"<start_of_turn>model\\n\" in model_part:\n        answer = model_part.split(\"<start_of_turn>model\\n\")[1].split(\"<end_of_turn>\")[0].strip()\n    else:\n        answer = model_part.strip()\n    \n    return answer","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example SQuAD passages and questions for testing\nexamples = [\n    {\n        \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n        \"question\": \"Which NFL team won Super Bowl 50?\",\n        \"reference_answer\": \"Denver Broncos\"\n    },\n    {\n        \"context\": \"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\",\n        \"question\": \"What is computational complexity theory a branch of?\",\n        \"reference_answer\": \"theory of computation\"\n    },\n    {\n        \"context\": \"Nikola Tesla (10 July 1856 – 7 January 1943) was a Serbian-American inventor, electrical engineer, mechanical engineer, and futurist best known for his contributions to the design of the modern alternating current (AC) electricity supply system. Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.\",\n        \"question\": \"When was Nikola Tesla born?\",\n        \"reference_answer\": \"10 July 1856\"\n    }\n]\n\n# Test the model on the examples\nfor idx, example in enumerate(examples):\n    print(f\"Example {idx+1}:\")\n    print(f\"Context: {example['context'][:100]}...\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Reference Answer: {example['reference_answer']}\")\n    \n    model_answer = answer_question(example['context'], example['question'])\n    print(f\"Model Answer: {model_answer}\")\n    print(\"-\" * 80)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Compare Gemma 2 vs Gemma 3 Performance\n\nNow that we've fine-tuned Gemma 3 on the SQuAD dataset, let's analyze the differences in performance compared to Gemma 2.","metadata":{}},{"cell_type":"code","source":"# Load the previously fine-tuned Gemma 2 model (if available)\n# Note: Adjust paths as needed\ngemma2_path = \"./results\"  # Path to your Gemma 2 fine-tuned model\n\ntry:\n    # Import necessary libraries for Gemma 2\n    from transformers import AutoModelForCausalLM\n    \n    # Load Gemma 2 model and tokenizer\n    gemma2_peft_config = PeftConfig.from_pretrained(gemma2_path)\n    gemma2_base_model = AutoModelForCausalLM.from_pretrained(\n        gemma2_peft_config.base_model_name_or_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    gemma2_model = PeftModel.from_pretrained(gemma2_base_model, gemma2_path)\n    gemma2_tokenizer = AutoTokenizer.from_pretrained(gemma2_path)\n    \n    def gemma2_answer_question(context, question):\n        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n        \n        inputs = gemma2_tokenizer(prompt, return_tensors=\"pt\").to(gemma2_model.device)\n        \n        with torch.no_grad():\n            outputs = gemma2_model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True\n            )\n        \n        generated_text = gemma2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = generated_text[len(prompt):].strip()\n        \n        return answer\n    \n    print(\"Successfully loaded Gemma 2 model for comparison\")\n    has_gemma2 = True\nexcept Exception as e:\n    print(f\"Couldn't load Gemma 2 model: {e}\")\n    has_gemma2 = False","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if has_gemma2:\n    # Compare Gemma 2 vs Gemma 3 on the examples\n    print(\"\\n==== COMPARISON: GEMMA 2 vs GEMMA 3 ====\\n\")\n    \n    for idx, example in enumerate(examples):\n        print(f\"Example {idx+1}:\")\n        print(f\"Question: {example['question']}\")\n        print(f\"Reference Answer: {example['reference_answer']}\")\n        \n        # Get answers from both models\n        gemma2_answer = gemma2_answer_question(example['context'], example['question'])\n        gemma3_answer = answer_question(example['context'], example['question'])\n        \n        print(f\"Gemma 2 Answer: {gemma2_answer}\")\n        print(f\"Gemma 3 Answer: {gemma3_answer}\")\n        print(\"-\" * 80)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Key Differences Between Gemma 2 and Gemma 3\n\nBased on the model implementations and fine-tuning process, here are some key differences between Gemma 2 and Gemma 3:\n\n1. **Vocabulary Size**: \n   - Gemma 2: 256,000 tokens\n   - Gemma 3: 262,144 tokens (larger vocabulary)\n\n2. **Architecture Changes**:\n   - Gemma 3 includes multimodal capabilities with the `Gemma3ForMultimodalLM` class\n   - Gemma 3 uses a different layer configuration (Gemma 3 4B has 34 layers vs. different configurations in Gemma 2)\n   - QK normalization is enabled by default in Gemma 3\n\n3. **Context Length**:\n   - Gemma 2: 8,192 tokens\n   - Gemma 3: 32,768 tokens (4x longer context window)\n\n4. **Attention Mechanism**:\n   - Gemma 3 uses interleaved local/global attention with larger window sizes\n   - Attention window sizes in Gemma 3 4B: [1024, 1024, 1024, 1024, 1024, 32768]\n\n5. **Model Dimensionality**:\n   - Different model dimensions and hidden layer sizes\n   - Gemma 3 4B has model_dim=2560 compared to Gemma 2 models\n\n6. **Chat Template**:\n   - Gemma 3 uses the `GEMMA_VLM` prompt wrapping style for multimodal capabilities\n\n7. **Performance Expectations**:\n   - Improved reasoning capabilities\n   - Better handling of longer contexts\n   - More robust performance on complex questions","metadata":{}}]}