{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 3 Fine-Tuning on SQuAD Dataset\n\nThis notebook demonstrates how to fine-tune the Gemma 3 4B model on the Stanford Question Answering Dataset (SQuAD).\n\n\n## Introducing the Gemma 3 4B-IT\n\n**Gemma 3** is Google's latest addition to its family of lightweight, state-of-the-art open AI models, designed to deliver high performance while being resource-efficient. The **4B Instruct** version of **Gemma 3** is tailored for **instruction-based tasks**, offering developers an accessible and powerful tool for creating intelligent applications.  \n\nAnnouncement: [Gemma 3 Blog Post](https://blog.google/technology/developers/gemma-3/)\n\nGemma 3 features a **transformer architecture** optimized with advanced techniques like **RoPE embeddings** and **GeGLU activations**, enabling sophisticated reasoning and text generation capabilities.\n\nKey Features:\n- **128K-token context window**: Allows processing and understanding of vast amounts of information.  \n- **Multilingual support**: Over **140 languages**, ideal for global applications.  \n- **Multimodal capabilities**: Supports text, images, and videos, enabling interactive AI solutions.  \n- **Edge device optimization**: Efficiently runs on consumer hardware with a single GPU, making it accessible for developers with limited resources.\n\nResources:\n- [Gemma 3 Model Overview](https://ai.google.dev/gemma/docs/core)  \n- [Gemma 3 Technical Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)  \n- [Gemma 3 Model Card](https://ai.google.dev/gemma/docs/core/model_card_3)","metadata":{}},{"cell_type":"markdown","source":"### Package Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:00:28.476844Z","iopub.execute_input":"2025-04-22T09:00:28.477065Z","iopub.status.idle":"2025-04-22T09:00:35.338351Z","shell.execute_reply.started":"2025-04-22T09:00:28.477042Z","shell.execute_reply":"2025-04-22T09:00:35.337634Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 294, done.\u001b[K\nremote: Counting objects: 100% (177/177), done.\u001b[K\nremote: Compressing objects: 100% (97/97), done.\u001b[K\nremote: Total 294 (delta 127), reused 80 (delta 80), pack-reused 117 (from 1)\u001b[K\nReceiving objects: 100% (294/294), 5.53 MiB | 19.66 MiB/s, done.\nResolving deltas: 100% (165/165), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install required packages\n# !pip3 install -q -U bitsandbytes\n# !pip3 install -q -U peft\n!pip3 install -q -U trl\n# !pip3 install -q -U accelerate\n# !pip3 install -q -U datasets\n# !pip3 install -q -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:03:14.204880Z","iopub.execute_input":"2025-04-22T09:03:14.205566Z","iopub.status.idle":"2025-04-22T09:04:35.161592Z","shell.execute_reply.started":"2025-04-22T09:03:14.205539Z","shell.execute_reply":"2025-04-22T09:04:35.160684Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Check Package Versions\n# !pip freeze | grep bitsandbytes\n# !pip freeze | grep peft\n# !pip freeze | grep trl\n# !pip freeze | grep accelerate\n# !pip freeze | grep datasets\n!pip freeze | grep transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:15:02.474527Z","iopub.execute_input":"2025-04-22T09:15:02.475078Z","iopub.status.idle":"2025-04-22T09:15:04.307951Z","shell.execute_reply.started":"2025-04-22T09:15:02.475058Z","shell.execute_reply":"2025-04-22T09:15:04.307222Z"}},"outputs":[{"name":"stdout","text":"sentence-transformers==3.4.1\ntransformers==4.51.1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Suppress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:57:30.280461Z","iopub.execute_input":"2025-04-22T08:57:30.280749Z","iopub.status.idle":"2025-04-22T08:57:30.284765Z","shell.execute_reply.started":"2025-04-22T08:57:30.280696Z","shell.execute_reply":"2025-04-22T08:57:30.284113Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport contextlib\nimport kagglehub\nimport sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \n\nimport torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\n\n# from transformers.models.gemma3 import Gemma3ForCausalLM\n\nfrom gemma.config import get_model_config\nfrom gemma.gemma3_model import Gemma3ForMultimodalLM\n\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig, PeftModel\nfrom trl import SFTTrainer, SFTConfig\n# import bitsandbytes as bnb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:04:35.163084Z","iopub.execute_input":"2025-04-22T09:04:35.163359Z","iopub.status.idle":"2025-04-22T09:04:39.979410Z","shell.execute_reply.started":"2025-04-22T09:04:35.163333Z","shell.execute_reply":"2025-04-22T09:04:39.978596Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### CUDA & GPU Checking","metadata":{}},{"cell_type":"code","source":"# Disable W&B logging for this run\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# Set Cuda Allocation\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use the first GPU\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenization parallelism\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:36:48.955884Z","iopub.execute_input":"2025-04-22T07:36:48.956729Z","iopub.status.idle":"2025-04-22T07:36:48.961440Z","shell.execute_reply.started":"2025-04-22T07:36:48.956694Z","shell.execute_reply":"2025-04-22T07:36:48.960724Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Choose variant and machine type\nVARIANT = '4b'\nMACHINE_TYPE = 'cuda'\nOUTPUT_LEN = 20\nMETHOD = 'it'\n\nweights_dir = kagglehub.model_download(f\"google/gemma-3/pytorch/gemma-3-{VARIANT}-{METHOD}/1\")\ntokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\nckpt_path = os.path.join(weights_dir, f'model.ckpt')\n\n# Set up model config.\nmodel_config = get_model_config(VARIANT)\nmodel_config.dtype = \"float32\" if MACHINE_TYPE == \"cpu\" else \"float16\"\nmodel_config.tokenizer = tokenizer_path\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Instantiate the model\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = Gemma3ForMultimodalLM(model_config)\n\n# For loading the model weights (Inference) ----------------- For Reference\n# device = torch.device(MACHINE_TYPE)\n# with _set_default_tensor_type(model_config.get_dtype()):\n#     model = Gemma3ForMultimodalLM(model_config)\n#     model.load_state_dict(torch.load(ckpt_path)['model_state_dict'])        \n#     model = model.to(device).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:07:13.533864Z","iopub.execute_input":"2025-04-22T09:07:13.534217Z","iopub.status.idle":"2025-04-22T09:07:18.757931Z","shell.execute_reply.started":"2025-04-22T09:07:13.534163Z","shell.execute_reply":"2025-04-22T09:07:18.757325Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3 -q --no-cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:21:36.227765Z","iopub.execute_input":"2025-04-22T09:21:36.228518Z","iopub.status.idle":"2025-04-22T09:22:10.120683Z","shell.execute_reply.started":"2025-04-22T09:21:36.228484Z","shell.execute_reply":"2025-04-22T09:22:10.119945Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nhf_token_name = \"HF_TOKEN_EG\"\nhf_key = UserSecretsClient().get_secret(hf_token_name)\nprint(f\"Successfully loaded {hf_token_name}!\")\n\nlogin(token = hf_key)\nprint(f\"Login with {hf_token_name} complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:22:15.250815Z","iopub.execute_input":"2025-04-22T09:22:15.251562Z","iopub.status.idle":"2025-04-22T09:22:15.532055Z","shell.execute_reply.started":"2025-04-22T09:22:15.251534Z","shell.execute_reply":"2025-04-22T09:22:15.531340Z"}},"outputs":[{"name":"stdout","text":"Successfully loaded HF_TOKEN_EG!\nLogin with HF_TOKEN_EG complete!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Load Gemma 3‑27B‑IT’s tokenizer\nMODEL_GEMMA = \"google/gemma-3-27b-it\"\ngemma_tokenizer = AutoTokenizer.from_pretrained(MODEL_GEMMA, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:22:37.253063Z","iopub.execute_input":"2025-04-22T09:22:37.253744Z","iopub.status.idle":"2025-04-22T09:22:41.131300Z","shell.execute_reply.started":"2025-04-22T09:22:37.253709Z","shell.execute_reply":"2025-04-22T09:22:41.130722Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b92f1bef81c49a49b91b23b9ce4b90e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5ce3bd55c440af81144cdb036d9070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10f23ebc56743949d01581d5be2b96e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec77f43423774ae08456686e26b05376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5aad1155fab40a3a493231f0b76bcaf"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def define_device():\n    \"\"\"Determine and return the optimal PyTorch device based on availability.\"\"\"\n    \n    print(f\"PyTorch version: {torch.__version__}\", end=\" -- \")\n\n    # Check if MPS (Metal Performance Shaders) is available for macOS\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        print(\"using MPS device on macOS\")\n        return torch.device(\"mps\")\n\n    # Check for CUDA availability\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"using {device}\")\n    return device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:37:22.483834Z","iopub.execute_input":"2025-04-22T07:37:22.484094Z","iopub.status.idle":"2025-04-22T07:37:22.488784Z","shell.execute_reply.started":"2025-04-22T07:37:22.484065Z","shell.execute_reply":"2025-04-22T07:37:22.487990Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 1. Load Gemma 3 Model and Tokenizer\n\n* If the GPU supports **bfloat16** (available on GPUs with Compute Capability **8.0+**), it is used for computations.  \n  Otherwise, **float16** is used as the default.  \n\n* **Device Selection:**  \n  * The function `define_device()` selects the best available device (**CPU, CUDA, or MPS**).  \n\n* **Model Initialization:**  \n  * The model is loaded with memory-efficient configurations, including `low_cpu_mem_usage=True`, and assigned to the selected device.  \n\n* **Tokenizer Setup:**  \n  * A **tokenizer** is initialized with a **maximum sequence length of 1024**.  \n  * The **end-of-sequence (EOS) token** is stored for later use.  ","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:39:39.235237Z","iopub.execute_input":"2025-04-22T07:39:39.235778Z","iopub.status.idle":"2025-04-22T07:39:39.259086Z","shell.execute_reply.started":"2025-04-22T07:39:39.235756Z","shell.execute_reply":"2025-04-22T07:39:39.258347Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-3/pytorch/gemma-3-4b-it/1/model.ckpt\n/kaggle/input/gemma-3/pytorch/gemma-3-4b-it/1/tokenizer.model\n/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Determine optimal computation dtype based on GPU capability\ncompute_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\nprint(f\"Using compute dtype {compute_dtype}\")\n\n# Select the best available device (CPU, CUDA, or MPS)\ndevice = define_device()\nprint(f\"Operating on {device}\")\n\n# Path to the pre-trained model\nGEMMA_PATH = \"/kaggle/input/gemma-3/pytorch/gemma-3-4b-it/1\"\n\n# Load the model with optimized settings\nmodel = Gemma3ForCausalLM.from_pretrained(\n    GEMMA_PATH,\n    torch_dtype=compute_dtype,\n    attn_implementation=\"eager\",\n    low_cpu_mem_usage=True,\n    device_map=device\n)\n\n# Define maximum sequence length for the tokenizer\nmax_seq_length = 1024\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    GEMMA_PATH, \n    max_seq_length=max_seq_length,\n    device_map=device\n)\n\n# Store the EOS token for later use\nEOS_TOKEN = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:40:07.768702Z","iopub.execute_input":"2025-04-22T07:40:07.769031Z","iopub.status.idle":"2025-04-22T07:40:07.803116Z","shell.execute_reply.started":"2025-04-22T07:40:07.769006Z","shell.execute_reply":"2025-04-22T07:40:07.802081Z"}},"outputs":[{"name":"stdout","text":"Using compute dtype torch.float16\nPyTorch version: 2.5.1+cu124 -- using cuda\nOperating on cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1085175908.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the model with optimized settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m model = Gemma3ForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mGEMMA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4258\u001b[0m             )\n\u001b[1;32m   4259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4260\u001b[0;31m         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[0m\u001b[1;32m   4261\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4262\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m    950\u001b[0m                 )\n\u001b[1;32m    951\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m    953\u001b[0m                     \u001b[0;34mf\"Error no file named {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                     \u001b[0;34mf\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or {FLAX_WEIGHTS_NAME} found in directory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /kaggle/input/gemma-3/pytorch/gemma-3-4b-it/1."],"ename":"OSError","evalue":"Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /kaggle/input/gemma-3/pytorch/gemma-3-4b-it/1.","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"## 2. Load and Prepare SQuAD Dataset","metadata":{}},{"cell_type":"code","source":"# Load SQuAD dataset\nsquad_dataset = load_dataset(\"squad\")\nprint(squad_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.776267Z","iopub.status.idle":"2025-04-22T07:06:28.776889Z","shell.execute_reply.started":"2025-04-22T07:06:28.776693Z","shell.execute_reply":"2025-04-22T07:06:28.776713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Format SQuAD examples for instruction fine-tuning\n# We'll use a specific format tailored for Gemma 3's chat template\n\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\nContext: {context}\\n\\nQuestion: {question}<end_of_turn>\\n\"\nMODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{answer}<end_of_turn>\\n\"\n\ndef format_squad_example(example):\n    user_prompt = USER_CHAT_TEMPLATE.format(\n        context=example[\"context\"],\n        question=example[\"question\"]\n    )\n    model_response = MODEL_CHAT_TEMPLATE.format(answer=example[\"answers\"][\"text\"][0])\n    return {\n        \"formatted_prompt\": user_prompt + model_response,\n        \"input\": user_prompt,\n        \"output\": model_response\n    }\n\n# Apply formatting to the dataset\ntrain_dataset = squad_dataset[\"train\"].map(format_squad_example)\nvalidation_dataset = squad_dataset[\"validation\"].map(format_squad_example)\n\n# Take a subset for faster experimentation\ntrain_subset = train_dataset.select(range(1000))  # Adjust as needed\nvalidation_subset = validation_dataset.select(range(100))  # Adjust as needed\n\nprint(f\"Training examples: {len(train_subset)}\")\nprint(f\"Validation examples: {len(validation_subset)}\")\n\n# Display an example\nprint(\"\\nExample of formatted data:\")\nprint(train_subset[0][\"formatted_prompt\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.778121Z","iopub.status.idle":"2025-04-22T07:06:28.778433Z","shell.execute_reply.started":"2025-04-22T07:06:28.778260Z","shell.execute_reply":"2025-04-22T07:06:28.778276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Prepare Model for Fine-Tuning with PEFT\n\nIn the next cell, we set everything up for fine-tuning the model. We configure and initialize a **Simple Fine-tuning Trainer (SFTTrainer)** for training the model using the **Parameter-Efficient Fine-Tuning (PEFT)** method. PEFT is efficient because it operates on a reduced number of parameters compared to the model's overall size. This method focuses on refining only a limited set of additional model parameters while keeping the majority of the pre-trained large language model (LLM) parameters fixed, significantly reducing computational and storage expenses. Additionally, PEFT helps mitigate **catastrophic forgetting**, a common issue when fine-tuning LLMs completely.\n\n### PEFTConfig:\nThe `peft_config` object specifies the parameters for PEFT. The following are some of the most important parameters:\n\n- **lora_alpha**: The learning rate for the LoRA update matrices.\n- **lora_dropout**: The dropout probability for the LoRA update matrices.\n- **r**: The rank of the LoRA update matrices.\n- **bias**: The type of bias to use. Possible values are: `none`, `additive`, and `learned`.\n- **task_type**: The task type the model is being trained for. Possible values are `CAUSAL_LM` and `MASKED_LM`.\n\n### TrainingArguments:\nThe `training_arguments` object specifies the parameters for training the model. The following are some key parameters:\n\n- **output_dir**: Directory where the training logs and checkpoints will be saved.\n- **num_train_epochs**: Number of epochs to train the model for.\n- **per_device_train_batch_size**: Number of samples in each batch on each device.\n- **gradient_accumulation_steps**: Number of batches to accumulate gradients before updating the model parameters.\n- **gradient_checkpointing**: Whether to use gradient checkpointing to reduce GPU memory usage.\n- **optim**: The optimizer used for training the model.\n- **save_steps**: The number of steps after which to save a checkpoint.\n- **logging_steps**: The number of steps after which to log the training metrics.\n- **learning_rate**: The learning rate for the optimizer.\n- **weight_decay**: The weight decay parameter for the optimizer.\n- **fp16**: Whether to use 16-bit floating-point precision.\n- **bf16**: Whether to use BFloat16 precision.\n- **max_grad_norm**: The maximum gradient norm.\n- **max_steps**: The maximum number of steps to train the model for.\n- **warmup_ratio**: Proportion of training steps to use for warming up the learning rate.\n- **group_by_length**: Whether to group the training samples by length.\n- **lr_scheduler_type**: The type of learning rate scheduler to use.\n- **report_to**: The tools to report the training metrics to.\n- **evaluation_strategy**: The strategy for evaluating the model during training.\n- **eval_steps**: Number of update steps between evaluations.\n- **eval_accumulation_steps**: Number of prediction steps to accumulate before moving the output to CPU.\n\n### SFTTrainer:\nThe `SFTTrainer` is a custom trainer class from the **TRL** library. It is used to fine-tune large language models using the PEFT method.\n\nThe `SFTTrainer` object is initialized with the following arguments:\n\n- **model**: The model to be trained.\n- **train_dataset**: The training dataset.\n- **eval_dataset**: The evaluation dataset.\n- **peft_config**: The PEFT configuration.\n- **tokenizer**: The tokenizer to use.\n- **args**: The training arguments.\n- **dataset_text_field**: The name of the text field in the dataset.\n- **packing**: Whether to pack the training samples.\n- **max_seq_length**: The maximum sequence length.\n\nOnce the `SFTTrainer` object is initialized, it can be used to train the model by calling the `train()` method.","metadata":{}},{"cell_type":"code","source":"# Apply LoRA configuration\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",  \n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    r=16,                         \n    lora_alpha=32,                \n    lora_dropout=0.1,\n    bias=\"none\"\n)\n\n# Add LoRA adapters to the model\nmodel = get_peft_model(model, peft_config)\n\n# Freeze all parameters except LoRA parameters\nfor name, param in model.named_parameters():\n    if \"lora\" not in name:\n        param.requires_grad = False  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.779550Z","iopub.status.idle":"2025-04-22T07:06:28.779840Z","shell.execute_reply.started":"2025-04-22T07:06:28.779689Z","shell.execute_reply":"2025-04-22T07:06:28.779702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Configure Training","metadata":{}},{"cell_type":"code","source":"# # Set training arguments\n# training_args = transformers.TrainingArguments(\n#     output_dir=\"./gemma3_squad_results\",\n#     eval_strategy=\"steps\",\n#     evaluation_strategy=\"steps\",  # More explicit parameter name\n#     per_device_train_batch_size=1,\n#     gradient_accumulation_steps=4,\n#     warmup_steps=2,\n#     max_steps=100,  # Adjust based on available time/resources\n#     learning_rate=2e-5,  # Slightly lower than with Gemma 2\n#     fp16=True if torch_dtype == torch.float16 else False,\n#     bf16=True if torch_dtype == torch.bfloat16 else False,\n#     optim=\"paged_adamw_8bit\",\n#     save_strategy=\"steps\",\n#     save_steps=50,\n#     eval_steps=25,\n#     logging_dir=\"./logs\",\n#     logging_steps=10,\n#     push_to_hub=False,\n#     report_to=\"none\",  # Disable reporting to wandb\n#     run_name=\"gemma3-squad-finetune\"\n# )\n\ntraining_arguments = SFTConfig(\n    output_dir=\"logs\",\n    num_train_epochs=3,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Use reentrant checkpointing\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    optim=\"adamw_torch_fused\",  # Use fused AdamW optimizer\n    save_steps=112,\n    load_best_model_at_end=True,\n    logging_steps=25,\n    learning_rate=2e-5,\n    weight_decay=0.001,\n    fp16=True if compute_dtype == torch.float16 else False,  # Use float16 precision\n    bf16=True if compute_dtype == torch.bfloat16 else False,  # Use bfloat16 precision\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=False,\n    evaluation_strategy=\"steps\",\n    eval_steps=112,\n    eval_accumulation_steps=1,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\",\n    max_seq_length=max_seq_length,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # Template with special tokens\n        \"append_concat_token\": True,  # Add EOS token as separator token\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.781021Z","iopub.status.idle":"2025-04-22T07:06:28.781275Z","shell.execute_reply.started":"2025-04-22T07:06:28.781131Z","shell.execute_reply":"2025-04-22T07:06:28.781140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Initialize SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_subset,\n    eval_dataset=validation_subset,\n    peft_config=peft_config,\n    processing_class=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.781911Z","iopub.status.idle":"2025-04-22T07:06:28.782170Z","shell.execute_reply.started":"2025-04-22T07:06:28.782053Z","shell.execute_reply":"2025-04-22T07:06:28.782067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Train the Model","metadata":{}},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.783430Z","iopub.status.idle":"2025-04-22T07:06:28.783678Z","shell.execute_reply.started":"2025-04-22T07:06:28.783564Z","shell.execute_reply":"2025-04-22T07:06:28.783577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel_save_path = \"./gemma3_LoRA_squad_finetuned\"\ntrainer.model.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.784384Z","iopub.status.idle":"2025-04-22T07:06:28.784614Z","shell.execute_reply.started":"2025-04-22T07:06:28.784488Z","shell.execute_reply":"2025-04-22T07:06:28.784496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Inference with the Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"# Load the fine-tuned model for inference\nfrom peft import PeftModel, PeftConfig\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(model_save_path)\n\n# Reload model with the fine-tuned weights\nwith _set_default_tensor_type(model_config.get_dtype()):\n    eval_model = Gemma3ForMultimodalLM(model_config)\n    eval_model.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n    \n# Load the PEFT model\neval_model = PeftModel.from_pretrained(eval_model, model_save_path)\neval_model = eval_model.to(device).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.786075Z","iopub.status.idle":"2025-04-22T07:06:28.786392Z","shell.execute_reply.started":"2025-04-22T07:06:28.786234Z","shell.execute_reply":"2025-04-22T07:06:28.786248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for question answering with the fine-tuned model\ndef answer_question(context, question, output_len=50):\n    user_prompt = USER_CHAT_TEMPLATE.format(context=context, question=question)\n    \n    # Tokenize input\n    inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate answer\n    with torch.no_grad():\n        outputs = eval_model.generate(\n            inputs.input_ids,\n            max_new_tokens=output_len,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    # Decode the generated text and extract the answer\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract the model's answer part\n    model_part = generated_text[len(user_prompt):]\n    \n    # Remove the model chat template if present\n    if \"<start_of_turn>model\\n\" in model_part:\n        answer = model_part.split(\"<start_of_turn>model\\n\")[1].split(\"<end_of_turn>\")[0].strip()\n    else:\n        answer = model_part.strip()\n    \n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.787536Z","iopub.status.idle":"2025-04-22T07:06:28.787929Z","shell.execute_reply.started":"2025-04-22T07:06:28.787732Z","shell.execute_reply":"2025-04-22T07:06:28.787748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example SQuAD passages and questions for testing\nexamples = [\n    {\n        \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n        \"question\": \"Which NFL team won Super Bowl 50?\",\n        \"reference_answer\": \"Denver Broncos\"\n    },\n    {\n        \"context\": \"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\",\n        \"question\": \"What is computational complexity theory a branch of?\",\n        \"reference_answer\": \"theory of computation\"\n    },\n    {\n        \"context\": \"Nikola Tesla (10 July 1856 – 7 January 1943) was a Serbian-American inventor, electrical engineer, mechanical engineer, and futurist best known for his contributions to the design of the modern alternating current (AC) electricity supply system. Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.\",\n        \"question\": \"When was Nikola Tesla born?\",\n        \"reference_answer\": \"10 July 1856\"\n    }\n]\n\n# Test the model on the examples\nfor idx, example in enumerate(examples):\n    print(f\"Example {idx+1}:\")\n    print(f\"Context: {example['context'][:100]}...\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Reference Answer: {example['reference_answer']}\")\n    \n    model_answer = answer_question(example['context'], example['question'])\n    print(f\"Model Answer: {model_answer}\")\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.789013Z","iopub.status.idle":"2025-04-22T07:06:28.789326Z","shell.execute_reply.started":"2025-04-22T07:06:28.789169Z","shell.execute_reply":"2025-04-22T07:06:28.789183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Compare Gemma 2 vs Gemma 3 Performance\n\nNow that we've fine-tuned Gemma 3 on the SQuAD dataset, let's analyze the differences in performance compared to Gemma 2.","metadata":{}},{"cell_type":"code","source":"# Load the previously fine-tuned Gemma 2 model (if available)\n# Note: Adjust paths as needed\ngemma2_path = \"./results\"  # Path to your Gemma 2 fine-tuned model\n\ntry:\n    # Import necessary libraries for Gemma 2\n    from transformers import AutoModelForCausalLM\n    \n    # Load Gemma 2 model and tokenizer\n    gemma2_peft_config = PeftConfig.from_pretrained(gemma2_path)\n    gemma2_base_model = AutoModelForCausalLM.from_pretrained(\n        gemma2_peft_config.base_model_name_or_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    gemma2_model = PeftModel.from_pretrained(gemma2_base_model, gemma2_path)\n    gemma2_tokenizer = AutoTokenizer.from_pretrained(gemma2_path)\n    \n    def gemma2_answer_question(context, question):\n        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n        \n        inputs = gemma2_tokenizer(prompt, return_tensors=\"pt\").to(gemma2_model.device)\n        \n        with torch.no_grad():\n            outputs = gemma2_model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True\n            )\n        \n        generated_text = gemma2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = generated_text[len(prompt):].strip()\n        \n        return answer\n    \n    print(\"Successfully loaded Gemma 2 model for comparison\")\n    has_gemma2 = True\nexcept Exception as e:\n    print(f\"Couldn't load Gemma 2 model: {e}\")\n    has_gemma2 = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.790393Z","iopub.status.idle":"2025-04-22T07:06:28.790616Z","shell.execute_reply.started":"2025-04-22T07:06:28.790513Z","shell.execute_reply":"2025-04-22T07:06:28.790523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if has_gemma2:\n    # Compare Gemma 2 vs Gemma 3 on the examples\n    print(\"\\n==== COMPARISON: GEMMA 2 vs GEMMA 3 ====\\n\")\n    \n    for idx, example in enumerate(examples):\n        print(f\"Example {idx+1}:\")\n        print(f\"Question: {example['question']}\")\n        print(f\"Reference Answer: {example['reference_answer']}\")\n        \n        # Get answers from both models\n        gemma2_answer = gemma2_answer_question(example['context'], example['question'])\n        gemma3_answer = answer_question(example['context'], example['question'])\n        \n        print(f\"Gemma 2 Answer: {gemma2_answer}\")\n        print(f\"Gemma 3 Answer: {gemma3_answer}\")\n        print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:06:28.792149Z","iopub.status.idle":"2025-04-22T07:06:28.792479Z","shell.execute_reply.started":"2025-04-22T07:06:28.792313Z","shell.execute_reply":"2025-04-22T07:06:28.792329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Key Differences Between Gemma 2 and Gemma 3\n\nBased on the model implementations and fine-tuning process, here are some key differences between Gemma 2 and Gemma 3:\n\n1. **Vocabulary Size**: \n   - Gemma 2: 256,000 tokens\n   - Gemma 3: 262,144 tokens (larger vocabulary)\n\n2. **Architecture Changes**:\n   - Gemma 3 includes multimodal capabilities with the `Gemma3ForMultimodalLM` class\n   - Gemma 3 uses a different layer configuration (Gemma 3 4B has 34 layers vs. different configurations in Gemma 2)\n   - QK normalization is enabled by default in Gemma 3\n\n3. **Context Length**:\n   - Gemma 2: 8,192 tokens\n   - Gemma 3: 32,768 tokens (4x longer context window)\n\n4. **Attention Mechanism**:\n   - Gemma 3 uses interleaved local/global attention with larger window sizes\n   - Attention window sizes in Gemma 3 4B: [1024, 1024, 1024, 1024, 1024, 32768]\n\n5. **Model Dimensionality**:\n   - Different model dimensions and hidden layer sizes\n   - Gemma 3 4B has model_dim=2560 compared to Gemma 2 models\n\n6. **Chat Template**:\n   - Gemma 3 uses the `GEMMA_VLM` prompt wrapping style for multimodal capabilities\n\n7. **Performance Expectations**:\n   - Improved reasoning capabilities\n   - Better handling of longer contexts\n   - More robust performance on complex questions","metadata":{}}]}