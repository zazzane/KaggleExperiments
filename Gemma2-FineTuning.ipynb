{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experimentation with Google's Gemma3 models\n\n### Fine Tuning Components\n- Supervised Fine Tuning (SFT)\n- Low Rank Adaptations (LoRA)\n\n### Dataset\n- Stamford's question answering dataset (link: `https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset/data`)","metadata":{}},{"cell_type":"markdown","source":"### Environment Setup","metadata":{}},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes\n!pip3 install -q -U peft\n!pip3 install -q -U trl\n!pip3 install -q -U accelerate\n!pip3 install -q -U datasets\n!pip3 install -q -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:23:40.079158Z","iopub.execute_input":"2025-04-21T08:23:40.079490Z","iopub.status.idle":"2025-04-21T08:24:15.309083Z","shell.execute_reply.started":"2025-04-21T08:23:40.079451Z","shell.execute_reply":"2025-04-21T08:24:15.308180Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip freeze | grep bitsandbytes\n!pip freeze | grep peft\n!pip freeze | grep trl\n!pip freeze | grep accelerate\n!pip freeze | grep datasets\n!pip freeze | grep transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:15.310381Z","iopub.execute_input":"2025-04-21T08:24:15.310714Z","iopub.status.idle":"2025-04-21T08:24:21.917041Z","shell.execute_reply.started":"2025-04-21T08:24:15.310684Z","shell.execute_reply":"2025-04-21T08:24:21.915577Z"}},"outputs":[{"name":"stdout","text":"bitsandbytes==0.45.5\npeft==0.15.2\nfastrlock==0.8.2\ntrl==0.16.1\naccelerate==1.6.0\ndatasets==3.5.0\ntensorflow-datasets==4.9.7\nvega-datasets==0.9.0\nsentence-transformers==3.3.1\ntransformers==4.51.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport transformers\nimport torch\nfrom google.colab import userdata\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig, GemmaTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset\nfrom transformers import Trainer, TrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:21.919171Z","iopub.execute_input":"2025-04-21T08:24:21.919524Z","iopub.status.idle":"2025-04-21T08:24:47.926047Z","shell.execute_reply.started":"2025-04-21T08:24:21.919499Z","shell.execute_reply":"2025-04-21T08:24:47.924322Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nhf_token_name = \"HF_TOKEN_EG\"\nhf_key = UserSecretsClient().get_secret(hf_token_name)\nprint(f\"Successfully loaded {hf_token_name}!\")\n\nlogin(token = hf_key)\nprint(f\"Login with {hf_token_name} complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:47.927804Z","iopub.execute_input":"2025-04-21T08:24:47.928157Z","iopub.status.idle":"2025-04-21T08:24:48.142740Z","shell.execute_reply.started":"2025-04-21T08:24:47.928116Z","shell.execute_reply":"2025-04-21T08:24:48.142073Z"}},"outputs":[{"name":"stdout","text":"Successfully loaded HF_TOKEN_EG!\nLogin with HF_TOKEN_EG complete!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:48.143528Z","iopub.execute_input":"2025-04-21T08:24:48.143767Z","iopub.status.idle":"2025-04-21T08:24:48.156372Z","shell.execute_reply.started":"2025-04-21T08:24:48.143749Z","shell.execute_reply":"2025-04-21T08:24:48.155785Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load SQuAD dataset\ndataset = load_dataset(\"squad\")\n\n# Use only a portion of the dataset\nsubset_size = 2500  # Adjust this value to control the subset size\ntrain_subset = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))\nvalidation_subset = dataset[\"validation\"].shuffle(seed=42).select(range(subset_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:48.157108Z","iopub.execute_input":"2025-04-21T08:24:48.157335Z","iopub.status.idle":"2025-04-21T08:24:50.873524Z","shell.execute_reply.started":"2025-04-21T08:24:48.157306Z","shell.execute_reply":"2025-04-21T08:24:50.872882Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605e7e1b51f04fc09945391f085131af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75ff4b814b346148acf599448db559c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5daf1a562d4b45bfc8f08f1b82a700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b526d355e2478e896e6f0cdff11c5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e6ec70eca34eb6958df51669d36879"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"# Adjust precision and attention based on GPU\nif torch.cuda.get_device_capability()[0] >= 8:\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\n    !pip install -qqq flash-attn\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"\n    \n# BitsAndBytes configuration for memory-efficient model loading\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_name = \"google/gemma-2-2b-it\"  \nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"cuda:0\", # do i need to make a custom device mapping dict (future for Gemma3)?\n    attn_implementation=attn_implementation,\n    offload_folder=\"./offload\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:24:50.874252Z","iopub.execute_input":"2025-04-21T08:24:50.874488Z","iopub.status.idle":"2025-04-21T08:25:21.901710Z","shell.execute_reply.started":"2025-04-21T08:24:50.874467Z","shell.execute_reply":"2025-04-21T08:25:21.900747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe2cb0f26b24db9b37b7d8e728d45be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811233d612ea49d9b2043e5302612f74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0027d0bc29de4629bf604201adbc85c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76d1472b71c40b8b8e173bfc4450e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4156115a794643d58ef4b34194c46fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"278f7139ce134bc2a026fbb897ad92c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a8f48edc0848cc9c85043a825577ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07df4612d03d40d69e40f8360ae46e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae3e6151724441aa680b2ed9989fafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ad9dd7747a84d9ea9807e36ac42788e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5a032191814d92be646000da7c3668"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Apply LoRA configuration\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",  \n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    r=16,                         \n    lora_alpha=32,                \n    lora_dropout=0.1,\n    bias=\"none\"\n)\n\n# Add LoRA adapters to the model\nmodel = get_peft_model(model, peft_config)\n\n# # Freeze all parameters except LoRA parameters\n# for name, param in model.named_parameters():\n#     if \"lora\" not in name:\n#         param.requires_grad = False  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:25:21.904443Z","iopub.execute_input":"2025-04-21T08:25:21.904716Z","iopub.status.idle":"2025-04-21T08:25:22.328132Z","shell.execute_reply.started":"2025-04-21T08:25:21.904694Z","shell.execute_reply":"2025-04-21T08:25:22.327458Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Tokenize the question\n    inputs = tokenizer(\n        examples[\"question\"], \n        max_length=256, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # Extract the first answer for each example in the batch\n    answers = [ans[\"text\"][0] if len(ans[\"text\"]) > 0 else \"\" for ans in examples[\"answers\"]]\n    \n    # Tokenize the answers\n    outputs = tokenizer(\n        answers, \n        max_length=256, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # Assign tokenized outputs as labels\n    inputs[\"labels\"] = outputs[\"input_ids\"]\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:25:22.329204Z","iopub.execute_input":"2025-04-21T08:25:22.329521Z","iopub.status.idle":"2025-04-21T08:25:28.519609Z","shell.execute_reply.started":"2025-04-21T08:25:22.329492Z","shell.execute_reply":"2025-04-21T08:25:28.518641Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Tokenize our train and validation datasets\n\ntokenized_train_subset = train_subset.map(preprocess_function, batched=True)\ntokenized_validation_subset = validation_subset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:25:28.521053Z","iopub.execute_input":"2025-04-21T08:25:28.521348Z","iopub.status.idle":"2025-04-21T08:25:30.521703Z","shell.execute_reply.started":"2025-04-21T08:25:28.521311Z","shell.execute_reply":"2025-04-21T08:25:30.520897Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b996506a1051429cb169bdd4507578ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e652cfbde354017bf34e2c896e21482"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(tokenized_train_subset[0]) # check one sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:25:30.522486Z","iopub.execute_input":"2025-04-21T08:25:30.522714Z","iopub.status.idle":"2025-04-21T08:25:30.527768Z","shell.execute_reply.started":"2025-04-21T08:25:30.522683Z","shell.execute_reply":"2025-04-21T08:25:30.527008Z"}},"outputs":[{"name":"stdout","text":"{'id': '573173d8497a881900248f0c', 'title': 'Egypt', 'context': 'The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.', 'question': 'What percentage of Egyptians polled support death penalty for those leaving Islam?', 'answers': {'text': ['84%'], 'answer_start': [468]}, 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1841, 12070, 576, 99102, 173486, 2676, 4168, 20749, 604, 1941, 10317, 21462, 235336], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 235321, 235310, 235358]}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# Disable W&B for this run only\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:25:30.528634Z","iopub.execute_input":"2025-04-21T08:25:30.528931Z","iopub.status.idle":"2025-04-21T08:25:30.563061Z","shell.execute_reply.started":"2025-04-21T08:25:30.528901Z","shell.execute_reply":"2025-04-21T08:25:30.562203Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=tokenized_train_subset,\n#     eval_dataset=tokenized_validation_subset,\n#     args=transformers.TrainingArguments(\n#         output_dir=\"./results\",\n#         eval_strategy=\"epoch\",\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=2,\n#         max_steps=100,\n#         learning_rate=2e-4, #\"5e-5\"\n#         fp16=True,\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#         logging_dir=\"./logs\",\n#         push_to_hub=False,\n#         weight_decay=0.01,\n#         run_name=\"gemma2-testrun\"\n#     ),\n#     peft_config=peft_config,\n# )\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n    learning_rate=5e-5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    warmup_steps=2,\n    max_steps=100,\n    weight_decay=0.01,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    push_to_hub=False,\n    run_name=\"gemma2-testrun\",  # Optional custom run name for W&B\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_subset,\n    eval_dataset=tokenized_validation_subset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:28:41.460075Z","iopub.execute_input":"2025-04-21T08:28:41.460433Z","iopub.status.idle":"2025-04-21T08:42:47.347675Z","shell.execute_reply.started":"2025-04-21T08:28:41.460405Z","shell.execute_reply":"2025-04-21T08:42:47.346833Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 13:56, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.536874</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=4.048531188964843, metrics={'train_runtime': 845.25, 'train_samples_per_second': 0.473, 'train_steps_per_second': 0.118, 'total_flos': 1256622863155200.0, 'train_loss': 4.048531188964843, 'epoch': 0.16})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"trainer.save_model(\"./results\")  # Save the model\ntokenizer.save_pretrained(\"./results\")  # Save the tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:44:21.359792Z","iopub.execute_input":"2025-04-21T08:44:21.360625Z","iopub.status.idle":"2025-04-21T08:44:22.137876Z","shell.execute_reply.started":"2025-04-21T08:44:21.360592Z","shell.execute_reply":"2025-04-21T08:44:22.136936Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./results/tokenizer_config.json',\n './results/special_tokens_map.json',\n './results/tokenizer.model',\n './results/added_tokens.json',\n './results/tokenizer.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\n# Load the fine-tuned model and tokenizer\nmodel_path = \"./results\"\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(model_path)\n\n# Load the base model specified in the PEFT config\nbase_model = AutoModelForCausalLM.from_pretrained(\n    peft_config.base_model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(base_model, model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Function for question answering with a causal language model\ndef answer_question(context, question):\n    # Format the input in the style used during training\n    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate answer\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    # Decode the generated text and extract the answer\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = generated_text[len(prompt):].strip()\n    \n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:50:29.492801Z","iopub.execute_input":"2025-04-21T08:50:29.493181Z","iopub.status.idle":"2025-04-21T08:50:37.235303Z","shell.execute_reply.started":"2025-04-21T08:50:29.493152Z","shell.execute_reply":"2025-04-21T08:50:37.234628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d82565e21f457da948b62ce65e06c5"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Example SQuAD passages and questions\nexamples = [\n    {\n        \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n        \"question\": \"Which NFL team won Super Bowl 50?\",\n        \"reference_answer\": \"Denver Broncos\"\n    },\n    {\n        \"context\": \"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\",\n        \"question\": \"What is computational complexity theory a branch of?\",\n        \"reference_answer\": \"theory of computation\"\n    },\n    {\n        \"context\": \"Nikola Tesla (10 July 1856 – 7 January 1943) was a Serbian-American inventor, electrical engineer, mechanical engineer, and futurist best known for his contributions to the design of the modern alternating current (AC) electricity supply system. Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.\",\n        \"question\": \"When was Nikola Tesla born?\",\n        \"reference_answer\": \"10 July 1856\"\n    }\n]\n\n# Test the model on the examples\nfor idx, example in enumerate(examples):\n    print(f\"Example {idx+1}:\")\n    print(f\"Context: {example['context'][:100]}...\")\n    print(f\"Question: {example['question']}\")\n    print(f\"Reference Answer: {example['reference_answer']}\")\n    \n    model_answer = answer_question(example['context'], example['question'])\n    print(f\"Model Answer: {model_answer}\")\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:51:20.305447Z","iopub.execute_input":"2025-04-21T08:51:20.305769Z","iopub.status.idle":"2025-04-21T08:51:22.646792Z","shell.execute_reply.started":"2025-04-21T08:51:20.305745Z","shell.execute_reply":"2025-04-21T08:51:22.646037Z"}},"outputs":[{"name":"stdout","text":"Example 1:\nContext: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\nQuestion: Which NFL team won Super Bowl 50?\nReference Answer: Denver Broncos\nModel Answer: \n--------------------------------------------------------------------------------\nExample 2:\nContext: Computational complexity theory is a branch of the theory of computation in theoretical computer sci...\nQuestion: What is computational complexity theory a branch of?\nReference Answer: theory of computation\nModel Answer: Computational complexity theory is a branch of theory of computation\n--------------------------------------------------------------------------------\nExample 3:\nContext: Nikola Tesla (10 July 1856 – 7 January 1943) was a Serbian-American inventor, electrical engineer, m...\nQuestion: When was Nikola Tesla born?\nReference Answer: 10 July 1856\nModel Answer: Nikola Tesla was born on July 10, 1856\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":19}]}