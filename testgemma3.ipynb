{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning Gemma 3 4B IT on SQuAD using PEFT LoRA on Kaggle\n\nThis notebook demonstrates how to fine-tune the `google/gemma-3-4b-it` model on the Stanford Question Answering Dataset (SQuAD) using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and 4-bit quantization. We'll leverage the `transformers`, `peft`, `trl`, `accelerate`, `bitsandbytes`, and `datasets` libraries within a Kaggle environment equipped with 2 T4 GPUs.\n\n**Goal:** Adapt the Gemma 3 instruction-tuned model to better handle question-answering tasks based on provided context, while optimizing resource usage.\n\n**Key Techniques:**\n\n1.  **Gemma 3 4B IT:** Utilizing Google's latest instruction-tuned model.\n2.  **SQuAD Dataset:** Training on a standard question-answering benchmark.\n3.  **PEFT & LoRA:** Significantly reducing the number of trainable parameters for efficient fine-tuning.\n4.  **4-bit Quantization:** Loading the base model in 4-bit precision to conserve GPU memory.\n5.  **SFTTrainer:** Using the `trl` library's trainer designed for supervised fine-tuning tasks.\n6.  **Kaggle Environment:** Utilizing the free T4 GPU resources.\n7.  **Loss Tracking:** Monitoring both training and validation loss during the process.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Kaggle Environment\n\n* **Enable GPUs:** Make sure to enable the GPU accelerator in your Kaggle notebook settings. Go to \"Settings\" -> \"Accelerator\" and select \"GPU T4 x2\".\n* **Hugging Face Token (Optional but Recommended):** While Gemma models might be accessible without login now, it's good practice to use a Hugging Face token, especially for potentially gated models or private use.\n    * Go to your Huggle Face account settings -> Access Tokens -> New token.\n    * In your Kaggle notebook, go to \"Add-ons\" -> \"Secrets\" and add your Hugging Face token with the name `HF_TOKEN`. The code below will attempt to log in using this secret.","metadata":{}},{"cell_type":"markdown","source":"## 2. Install Libraries\n\nWe need to install the necessary libraries from Hugging Face and other dependencies.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Install Libraries ---\n!pip install -q -U transformers datasets accelerate peft trl bitsandbytes huggingface_hub packaging ninja\n\n# The ninja package is often needed as a build dependency for bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:18:37.860456Z","iopub.execute_input":"2025-04-28T09:18:37.861219Z","iopub.status.idle":"2025-04-28T09:20:04.961762Z","shell.execute_reply.started":"2025-04-28T09:18:37.861192Z","shell.execute_reply":"2025-04-28T09:20:04.960936Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nlangchain-core 0.3.35 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip freeze | grep transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:20:04.963128Z","iopub.execute_input":"2025-04-28T09:20:04.963350Z","iopub.status.idle":"2025-04-28T09:20:06.648213Z","shell.execute_reply.started":"2025-04-28T09:20:04.963329Z","shell.execute_reply":"2025-04-28T09:20:06.647308Z"}},"outputs":[{"name":"stdout","text":"sentence-transformers==3.4.1\ntransformers==4.51.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Import Libraries & Login","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Imports and Login ---\nimport os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom trl import SFTTrainer, SFTConfig\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient # To access HF_TOKEN\n\n# --- Configuration ---\n# Attempt to log in to Hugging Face Hub\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN_EG\")\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face Hub.\")\nexcept Exception as e:\n    print(f\"Could not log in to Hugging Face Hub. Using public access. Error: {e}\")\n\n# Model ID\nmodel_id = \"google/gemma-3-4b-it\"\n\n# Dataset ID (Using Hugging Face's dataset identifier for SQuAD)\ndataset_name = \"squad\"\n\n# PEFT/LoRA Configuration\nlora_r = 16 # Rank of the LoRA matrices\nlora_alpha = 32 # Alpha parameter for LoRA scaling\nlora_dropout = 0.05 # Dropout probability for LoRA layers\n# Target modules can vary based on the model architecture.\n# Common targets for Gemma-like models include query, key, value, and output projections.\n# Inspecting model.named_modules() can help identify them.\n# Let's target common projection layers.\nlora_target_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n    \"down_proj\",\n]\n\n# BitsAndBytes Configuration (for 4-bit quantization)\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\" # Use bfloat16 for faster computation\nbnb_4bit_quant_type = \"nf4\" # Use NF4 quantization type for better precision\nuse_nested_quant = False # Activate nested quantization for more memory savings\n\n# TrainingArguments Configuration\noutput_dir = \"./gemma3-squad-finetuned\" # Directory to save the trained model adapters\nnum_train_epochs = 1 # Number of training epochs (adjust as needed)\nper_device_train_batch_size = 4 # Batch size per GPU\nper_device_eval_batch_size = 4 # Batch size for evaluation\ngradient_accumulation_steps = 2 # Number of steps to accumulate gradients before updating weights (effective batch size = 2 * 4 * 2 = 16)\ngradient_checkpointing = True # Use gradient checkpointing to save memory\noptim = \"paged_adamw_32bit\" # Use paged AdamW optimizer for memory efficiency\nlearning_rate = 2e-5 # Learning rate\nweight_decay = 0.001 # Weight decay for regularization\nmax_grad_norm = 0.3 # Gradient clipping threshold\nmax_steps = -1 # Maximum number of training steps (-1 for epoch-based training)\nwarmup_ratio = 0.03 # Ratio of steps for learning rate warmup\nlr_scheduler_type = \"constant\" # Learning rate scheduler type\nlogging_steps = 25 # Log training loss every 25 steps\neval_steps = 50 # Evaluate on validation set every 50 steps\nsave_steps = 50 # Save checkpoint every 50 steps\nevaluation_strategy = \"steps\" # Evaluate during training at `eval_steps`\nsave_strategy = \"steps\" # Save checkpoint during training at `save_steps`\nsave_total_limit = 2 # Keep only the last 2 checkpoints\nload_best_model_at_end = True # Load the best model found during training at the end\nreport_to = \"tensorboard\" # Log metrics to TensorBoard (useful in Kaggle)\n\n# SFTTrainer Configuration\nmax_seq_length = 512 # Maximum sequence length for tokenization\npacking = False # Whether to pack multiple sequences into one sample (set to False for QA)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:20:06.649444Z","iopub.execute_input":"2025-04-28T09:20:06.649740Z","iopub.status.idle":"2025-04-28T09:20:33.690542Z","shell.execute_reply.started":"2025-04-28T09:20:06.649708Z","shell.execute_reply":"2025-04-28T09:20:33.689760Z"}},"outputs":[{"name":"stderr","text":"2025-04-28 09:20:18.765405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745832018.975578      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745832019.038435      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged into Hugging Face Hub.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Load Dataset and Preprocess\n\nWe'll load the SQuAD dataset and preprocess it into a format suitable for `SFTTrainer`. SFTTrainer expects a single text column containing the full input and output sequence. We will format each SQuAD example (context, question, answer) into a prompt that instructs the model to answer the question based on the context.\n\n**Formatting Strategy:**\n\nWe'll use a simple template:\n\n```\n<start_of_turn>user\nContext: [context]\nQuestion: [question]\nAnswer:<end_of_turn>\n<start_of_turn>model\n[answer]<end_of_turn>\n```\n\nThis follows the instruction-following format Gemma expects.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load and Preprocess Dataset ---\nfrom datasets import load_dataset\n\n# Load SQuAD dataset\ndataset =  load_dataset(dataset_name, split='train') # Load the training split\n\n# SQuAD answers are structured. We need the first answer text.\ndef format_squad_example(example):\n    # Ensure 'answers' field exists and has 'text'\n    if 'answers' in example and 'text' in example['answers'] and len(example['answers']['text']) > 0:\n        context = example['context']\n        question = example['question']\n        answer = example['answers']['text'][0] # Take the first answer\n\n        # Create the formatted text string using the model's chat template structure\n        # Note: We are using a simplified template here. For best results,\n        # using the tokenizer's apply_chat_template might be preferable if available\n        # and correctly configured for the task.\n        formatted_text = f\"<start_of_turn>user\\nContext: {context}\\nQuestion: {question}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n{answer}<end_of_turn>\"\n        return {\"text\": formatted_text}\n    else:\n        # Handle cases where 'answers' might be missing or empty\n        # Returning None or an empty dict signals to filter this example out\n        return None\n\n\n# Apply the formatting function\n# We use map with batched=False as the logic is per-example.\n# remove_columns keeps only the new 'text' column SFTTrainer needs.\n# We also filter out examples that couldn't be formatted (returned None).\nformatted_dataset = dataset.map(format_squad_example, remove_columns=list(dataset.features))\nformatted_dataset = formatted_dataset.filter(lambda example: example['text'] is not None)\n\n# Take subset for faster experimentation (avoid OutOfMemoryError)\nformatted_dataset = formatted_dataset.select(range(5000))\n\n\n# Split the dataset into training and validation sets\n# SQuAD doesn't have a predefined validation split in the same format,\n# so we create one from the training data.\ntrain_test_split = formatted_dataset.train_test_split(test_size=0.1) # 10% for validation\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(eval_dataset)}\")\nprint(\"\\nSample formatted example:\")\nprint(train_dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:20:33.691999Z","iopub.execute_input":"2025-04-28T09:20:33.692371Z","iopub.status.idle":"2025-04-28T09:20:43.966311Z","shell.execute_reply.started":"2025-04-28T09:20:33.692342Z","shell.execute_reply":"2025-04-28T09:20:43.965716Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d82314f0274fae93641724d35fe34b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99aa0260e46a43539344ca6ae7dff579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be150a764bd34a3d8460ada8006e5fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d7b89391b1442fa9e8ee2c0980d210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83b571081554d1298fb02170569f703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78199e6c06a54a49880f828f3a4188a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e57c9d9eae466592f75f7ec6b26186"}},"metadata":{}},{"name":"stdout","text":"Training dataset size: 4500\nValidation dataset size: 500\n\nSample formatted example:\n<start_of_turn>user\nContext: The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's Sonata in F minor for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Memorial in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.\nQuestion: For whose benefit was the first of these concerts performed for on 2 April 1833?\nAnswer:<end_of_turn>\n<start_of_turn>model\nHarriet Smithson<end_of_turn>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. Load Model and Tokenizer\n\nNow, we load the Gemma 3 4B IT model and its tokenizer. We apply the 4-bit quantization configuration during model loading using `BitsAndBytesConfig`.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Model and Tokenizer ---\n\n# Load BitsAndBytes configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\n# create custom device dictionary\ncustom_device_map = {\n    # Vision Tower and Projector on GPU 0\n    \"vision_tower\": 0,\n    \"multi_modal_projector\": 0,\n\n    # Language Model Components\n    \"language_model.model.embed_tokens\": 0,\n    **{f\"language_model.model.layers.{i}\": 0 for i in range(24)}, # LLM layers 0-16 on GPU 0\n    **{f\"language_model.model.layers.{i}\": 1 for i in range(24, 33)}, # LLM layers 17-33 on GPU 1\n    \"language_model.model.norm\": 1,\n    \"language_model.lm_head\": 1,\n}\n \nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Automatically distribute model across GPUs ------------- # KIV cuda mapping, might cause Tensor error at validation\n    trust_remote_code=True, # Gemma 3 might require this\n    attn_implementation='eager' # 'sdpa' not compatible\n)\n\nmodel.config.use_cache = False  # Disable cache for training\nmodel.config.pretraining_tp = 1  # Set tensor parallelism degree (1 for no TP)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n# Set padding token and side. For Gemma, the EOS token is often used as the PAD token.\n# Check tokenizer config or documentation if unsure. Let's assume EOS for now.\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # \"right\" is common for causal models, ensures labels are not padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:21:30.490028Z","iopub.execute_input":"2025-04-28T09:21:30.490722Z","iopub.status.idle":"2025-04-28T09:22:11.720288Z","shell.execute_reply.started":"2025-04-28T09:21:30.490683Z","shell.execute_reply":"2025-04-28T09:22:11.719619Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201dc2962fa64fdc9e2e28918b5942db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e6b481093c42e69c96434da33bb494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484c423b58204b00893e033e051891e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b71b78a9c14a5e9dc74eb5ccdcdac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc51dd80b20443a28323006925ba708f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70dd04015f74860a9fd7569625cd748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8164fa578e7848e1aa2424f21ef17db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43522175978d49dd9c51f5d2f5d93583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6770dd01a6244aabde2b80ec48a5e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d22e0aebc1f0400483be141012a4c56c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1085c507ebc46f4a12f3734b911fd03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614a0426545f47c5975a66881c41b05b"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## 6. Configure PEFT (LoRA)\n\nWe integrate PEFT into the loaded model. `prepare_model_for_kbit_training` prepares the quantized model for PEFT, and `get_peft_model` applies the LoRA configuration.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure PEFT ---\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules=lora_target_modules,\n    bias=\"none\", # Typically set to 'none' for LoRA\n    task_type=\"CAUSAL_LM\",\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, peft_config)\n\n# Freeze unnecessary LoRA layers for Vision Model\nfor count, (name,param) in enumerate(model.named_parameters()):\n    if \"vision\" in name and \"lora\" in name:\n        param.requires_grad = False\n    # print(count,name,param.requires_grad)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:22:26.995319Z","iopub.execute_input":"2025-04-28T09:22:26.995911Z","iopub.status.idle":"2025-04-28T09:22:27.710078Z","shell.execute_reply.started":"2025-04-28T09:22:26.995886Z","shell.execute_reply":"2025-04-28T09:22:27.709479Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"trainable params: 29,802,496 || all params: 4,332,867,952 || trainable%: 0.6878\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:22:34.797535Z","iopub.execute_input":"2025-04-28T09:22:34.798300Z","iopub.status.idle":"2025-04-28T09:22:34.825457Z","shell.execute_reply.started":"2025-04-28T09:22:34.798253Z","shell.execute_reply":"2025-04-28T09:22:34.824701Z"},"scrolled":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma3ForConditionalGeneration(\n      (vision_tower): SiglipVisionModel(\n        (vision_model): SiglipVisionTransformer(\n          (embeddings): SiglipVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n            (position_embedding): Embedding(4096, 1152)\n          )\n          (encoder): SiglipEncoder(\n            (layers): ModuleList(\n              (0-26): 27 x SiglipEncoderLayer(\n                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                (self_attn): SiglipAttention(\n                  (k_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (q_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1152, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1152, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n                )\n                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n                (mlp): SiglipMLP(\n                  (activation_fn): PytorchGELUTanh()\n                  (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n                  (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n                )\n              )\n            )\n          )\n          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (multi_modal_projector): Gemma3MultiModalProjector(\n        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n      )\n      (language_model): Gemma3ForCausalLM(\n        (model): Gemma3TextModel(\n          (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n          (layers): ModuleList(\n            (0-33): 34 x Gemma3DecoderLayer(\n              (self_attn): Gemma3Attention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n              )\n              (mlp): Gemma3MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2560, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=10240, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=10240, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2560, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): PytorchGELUTanh()\n              )\n              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n            )\n          )\n          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n          (rotary_emb): Gemma3RotaryEmbedding()\n          (rotary_emb_local): Gemma3RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 7. Configure Training Arguments\n\nWe define the training parameters using `transformers.TrainingArguments`. This includes settings for batch size, learning rate, number of epochs, saving frequency, evaluation frequency, and more.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure Training Arguments ---\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=False, # fp16 not used with 4-bit, bf16 determined by compute_dtype\n    bf16=True if bnb_4bit_compute_dtype == 'bfloat16' and torch.cuda.is_bf16_supported() else False, # Use bf16 if supported for compute\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True, # Group sequences of similar length for efficiency\n    lr_scheduler_type=lr_scheduler_type,\n    logging_steps=logging_steps,\n    eval_strategy=evaluation_strategy,\n    eval_steps=eval_steps,\n    save_strategy=save_strategy,\n    save_steps=save_steps,\n    save_total_limit=save_total_limit,\n    load_best_model_at_end=load_best_model_at_end,\n    metric_for_best_model=\"eval_loss\", # Use eval loss to determine the best model\n    greater_is_better=False, # Lower eval loss is better\n    report_to=report_to,\n    push_to_hub=False, # Set to True if you want to push the adapter to Hub\n    gradient_checkpointing=gradient_checkpointing,\n    # ddp_find_unused_parameters=False, # Might be needed in some multi-GPU setups\n    dataset_text_field=\"text\", # The column containing our formatted text ###\n    max_seq_length=max_seq_length,  ###\n    packing=packing  ###\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:22:47.875503Z","iopub.execute_input":"2025-04-28T09:22:47.876119Z","iopub.status.idle":"2025-04-28T09:22:47.910288Z","shell.execute_reply.started":"2025-04-28T09:22:47.876095Z","shell.execute_reply":"2025-04-28T09:22:47.909470Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 8. Initialize SFTTrainer and Start Training\n\nWe create an instance of `SFTTrainer`, passing the model, datasets, PEFT config, tokenizer, training arguments, and other relevant parameters. Then, we start the training process. The trainer will automatically handle the training loop, evaluation, logging (including training and validation loss), and saving checkpoints.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Initialize Trainer and Train ---\n\n# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    # dataset_text_field=\"text\", # The column containing our formatted text\n    # max_seq_length=max_seq_length,\n    processing_class=tokenizer,\n    args=training_arguments,\n    # packing=packing,\n)\n\n# Start training\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training finished.\")\n\n# Save the final adapter model\nfinal_adapter_dir = os.path.join(output_dir, \"final_adapter\")\ntrainer.model.save_pretrained(final_adapter_dir)\nprint(f\"Final PEFT adapter model saved to {final_adapter_dir}\")\n\n# --- Optional: Clean up memory ---\n# del model\n# del trainer\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:22:55.013662Z","iopub.execute_input":"2025-04-28T09:22:55.014485Z","iopub.status.idle":"2025-04-28T09:23:45.466552Z","shell.execute_reply.started":"2025-04-28T09:22:55.014461Z","shell.execute_reply":"2025-04-28T09:23:45.465364Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04127bb3504469f9b0f14a4d05dc461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69bb8e7753e4d16ac8f8c76b3d3fbd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e0d0d7d807f472282baccab494182c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef29d3f41c03438eab9bd5c45221211a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79cbd5fa24244ebb8a8eb9f2b125f076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb58d78afbe843588936a0277b6c4314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3553b06192754bba8982c742d2f8d1aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b81e3131c934aaa8c9af4601d7a87bf"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/562 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2043269836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \"\"\"\n\u001b[1;32m    653\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         (loss, outputs) = super().compute_loss(\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0;31m# we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mshift_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m                 \u001b[0mshift_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshift_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m                 \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshift_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## 9. Evaluation and Inference (Optional)\n\nAfter training, the `trainer` object holds the training history, including training and validation loss, which can be accessed via `trainer.state.log_history`. TensorBoard logs will also be available in the `output_dir`.\n\nYou can also load the trained adapter model and perform inference.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Trained Model and Inference (Example) ---\nfrom peft import PeftModel\nimport torch\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- Reload Base Model and Tokenizer with Quantization ---\n# Ensure you reload the model in the same way you trained it (with quantization)\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# --- Load the PEFT Adapter ---\n# Use the path where the best model adapter was saved by the trainer\n# If load_best_model_at_end=True, the trainer saves it automatically.\n# Check the output_dir for checkpoint folders. Let's assume the final one is best for this example.\nadapter_path = \"/kaggle/working/gemma3-squad-finetuned/final_adapter\" # Or path to the best checkpoint, e.g., f\"{output_dir}/checkpoint-XXX\"\n\n# Load the LoRA adapter onto the base model\nmodel_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\nmodel_with_adapter = model_with_adapter.eval() # Set to evaluation mode\n\nprint(\"Loaded base model and adapter for inference.\")\n\n# --- Create Inference Pipeline ---\npipe = pipeline(\n    task=\"text-generation\",\n    model=model_with_adapter,\n    tokenizer=tokenizer,\n    max_new_tokens=50, # Limit the number of generated tokens (answer length)\n    # temperature=0.7, # Adjust creativity\n    # top_p=0.9,       # Use nucleus sampling\n    # repetition_penalty=1.1 # Penalize repetition\n)\n\n# --- Example Inference ---\n# Take an example from the original SQuAD validation set (or create one)\n# Note: Use the *original* SQuAD format here, not the training format.\n# The model expects the prompt format we used during fine-tuning.\n\n# Example from SQuAD dev set (you might need to load it separately)\ncontext_example = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ('Norman' comes from 'Norseman') raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\"\nquestion_example = \"Who were the Normans descended from?\"\n\n# Format the prompt like in training\nprompt = f\"<start_of_turn>user\\nContext: {context_example}\\nQuestion: {question_example}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n\"\n\n# Generate the answer\nresult = pipe(prompt)\n\nprint(\"\\n--- Inference Example ---\")\nprint(f\"Context: {context_example}\")\nprint(f\"Question: {question_example}\")\nprint(\"\\nGenerated Answer (following prompt):\")\n# The output includes the prompt, so we display the full generated text.\n# You might want to parse out just the answer part based on the <end_of_turn> token.\nprint(result[0]['generated_text'])\n\n# Parse the generated answer (simple approach)\ngenerated_full_text = result[0]['generated_text']\nanswer_start_tag = \"<start_of_turn>model\\n\"\nanswer_end_tag = \"<end_of_turn>\"\n\nstart_index = generated_full_text.find(answer_start_tag)\nif start_index != -1:\n    start_index += len(answer_start_tag)\n    end_index = generated_full_text.find(answer_end_tag, start_index)\n    if end_index != -1:\n        parsed_answer = generated_full_text[start_index:end_index].strip()\n        print(f\"\\nParsed Answer: {parsed_answer}\")\n    else:\n        # If end token not found, maybe take text after start token\n        parsed_answer = generated_full_text[start_index:].strip()\n        print(f\"\\nParsed Answer (end token not found): {parsed_answer}\")\nelse:\n    print(\"\\nCould not parse the answer using the expected format.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Conclusion\n\nThis notebook provided a comprehensive walkthrough for fine-tuning the Gemma 3 4B IT model on the SQuAD dataset using PEFT LoRA and 4-bit quantization within a Kaggle environment.\n\n**Key Takeaways:**\n\n* We successfully configured and ran a fine-tuning job using `SFTTrainer`.\n* PEFT LoRA allowed training with significantly fewer parameters.\n* 4-bit quantization drastically reduced memory requirements, making it feasible on T4 GPUs.\n* The training process tracked both training and validation loss, allowing monitoring of overfitting and model performance.\n* The final adapter model can be saved and used for inference on question-answering tasks formatted similarly to the training data.\n\n**Next Steps:**\n\n* Experiment with hyperparameters (learning rate, batch size, LoRA rank/alpha, number of epochs).\n* Try different prompt formatting strategies.\n* Evaluate the fine-tuned model using standard QA metrics (F1, Exact Match) on the SQuAD development set (requires custom evaluation logic).\n* Push the final adapter model to the Hugging Face Hub for easy sharing and reuse.","metadata":{}}]}