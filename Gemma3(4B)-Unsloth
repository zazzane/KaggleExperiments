{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 3 Fine Tuning via Unsloth library\n\n- This will be a fine tuning using public dataset to simulate in-domain fine tuning\n- Using the 4B model as a reference for training the 12B and 27B 'it' (instruction tuned) models provided by Unsloth\n\n### Why Unsloth?\n- Unsloth makes Gemma 3 (12B) finetuning 1.6x faster, use 60% less VRAM, and enables 6x longer than environments with Flash Attention 2 on a 48GB GPU. (ref: https://unsloth.ai/blog/gemma3#fixes)\n\n1. USP's\n   - Unsloth utilises dynamic 4-bit quants for increased accuracy\n   - Future scaling to Group Relative Policy Optimisation (GRPO)\n3. Gemma 3 Quirks\n   - Using FP16 on T4 GPU's provided by Kaggle results in gradients & activations reaching infinity\n   - Newer GPU's with BF16 tensorcores will not face this issue","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Library Installation","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install git+https://github.com/huggingface/transformers.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:128\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup Unsloth\n- includes LoRA adapters","metadata":{}},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n\n    # Other popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-4b-it\",\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Include the LoRA declaration for faster training","metadata":{}},{"cell_type":"code","source":"model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, # Turn off for just text!\n    finetune_language_layers   = True,  # Should leave on!\n    finetune_attention_modules = True,  # Attention good for GRPO\n    finetune_mlp_modules       = True,  # SHould leave on always!\n\n    r = 8,           # Larger = higher accuracy, but might overfit\n    lora_alpha = 8,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation\n1. Since `Gemma-3` has its own chat template, we can access it with Unsloth. This helps to format conversation styles.\n2. Dataset used is the `Maxime Labonne's FineTome-100k` dataset, Gemma-3 renders multi turn conversations like below:\n\n```\n<bos><start_of_turn>user\nHello!<end_of_turn>\n<start_of_turn>model\nHey there!<end_of_turn>\n```","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_data_formats\ndataset = standardize_data_formats(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check data\ndataset[100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one.","metadata":{}},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n   convos = examples[\"conversations\"]\n   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n   return { \"text\" : texts, }\n\ndataset = dataset.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[100][\"text\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = None, # Can set up evaluation!\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 30,\n        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Use this for WandB etc\n        dataset_num_proc=2,\n        fp16=False\n    ),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use Unsloths `train_on_completions` method to train only the assistant outputs and ignore loss on user's inputs. This will help improve finetuning accuracy.","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<start_of_turn>user\\n\",\n    response_part = \"<start_of_turn>model\\n\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"check if instruction part is properly masked - i.e. only one `<bos>` token in sample","metadata":{}},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"check if masked out example is correct","metadata":{}},{"cell_type":"code","source":"tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Show GPU memory Stats","metadata":{}},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Start Training","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference\n- According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\" : \"text\",\n        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n    }]\n}]\ntext = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n)\noutputs = model.generate(\n    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 64, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save model to FP16 vLLM\n- save it in the folder `gemma-3-finetune-unsloth`","metadata":{}},{"cell_type":"code","source":"model.save_pretrained_merged(\"gemma-3-4b-it-finetune-unsloth\", tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}