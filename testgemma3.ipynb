{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning Gemma 3 4B IT on SQuAD using PEFT LoRA on Kaggle\n\nThis notebook demonstrates how to fine-tune the `google/gemma-3-4b-it` model on the Stanford Question Answering Dataset (SQuAD) using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and 4-bit quantization. We'll leverage the `transformers`, `peft`, `trl`, `accelerate`, `bitsandbytes`, and `datasets` libraries within a Kaggle environment equipped with 2 T4 GPUs.\n\n**Goal:** Adapt the Gemma 3 instruction-tuned model to better handle question-answering tasks based on provided context, while optimizing resource usage.\n\n**Key Techniques:**\n\n1.  **Gemma 3 4B IT:** Utilizing Google's latest instruction-tuned model.\n2.  **SQuAD Dataset:** Training on a standard question-answering benchmark.\n3.  **PEFT & LoRA:** Significantly reducing the number of trainable parameters for efficient fine-tuning.\n4.  **4-bit Quantization:** Loading the base model in 4-bit precision to conserve GPU memory.\n5.  **SFTTrainer:** Using the `trl` library's trainer designed for supervised fine-tuning tasks.\n6.  **Kaggle Environment:** Utilizing the free T4 GPU resources.\n7.  **Loss Tracking:** Monitoring both training and validation loss during the process.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Kaggle Environment\n\n* **Enable GPUs:** Make sure to enable the GPU accelerator in your Kaggle notebook settings. Go to \"Settings\" -> \"Accelerator\" and select \"GPU T4 x2\".\n* **Hugging Face Token (Optional but Recommended):** While Gemma models might be accessible without login now, it's good practice to use a Hugging Face token, especially for potentially gated models or private use.\n    * Go to your Huggle Face account settings -> Access Tokens -> New token.\n    * In your Kaggle notebook, go to \"Add-ons\" -> \"Secrets\" and add your Hugging Face token with the name `HF_TOKEN`. The code below will attempt to log in using this secret.","metadata":{}},{"cell_type":"markdown","source":"## 2. Install Libraries\n\nWe need to install the necessary libraries from Hugging Face and other dependencies.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Install Libraries ---\n!pip install -q -U transformers datasets accelerate peft trl bitsandbytes huggingface_hub packaging ninja\n\n# The ninja package is often needed as a build dependency for bitsandbytes","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Import Libraries & Login","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Imports and Login ---\nimport os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom trl import SFTTrainer\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient # To access HF_TOKEN if set\n\n# --- Configuration ---\n# Attempt to log in to Hugging Face Hub\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face Hub.\")\nexcept Exception as e:\n    print(f\"Could not log in to Hugging Face Hub. Using public access. Error: {e}\")\n\n# Model ID\nmodel_id = \"google/gemma-3-4b-it\"\n\n# Dataset ID (Using Hugging Face's dataset identifier for SQuAD)\ndataset_name = \"squad\"\n\n# PEFT/LoRA Configuration\nlora_r = 16 # Rank of the LoRA matrices\nlora_alpha = 32 # Alpha parameter for LoRA scaling\nlora_dropout = 0.05 # Dropout probability for LoRA layers\n# Target modules can vary based on the model architecture.\n# Common targets for Gemma-like models include query, key, value, and output projections.\n# Inspecting model.named_modules() can help identify them.\n# Let's target common projection layers.\nlora_target_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n    \"down_proj\",\n]\n\n# BitsAndBytes Configuration (for 4-bit quantization)\nuse_4bit = True\nbnb_4bit_compute_dtype = \"bfloat16\" # Use bfloat16 for faster computation\nbnb_4bit_quant_type = \"nf4\" # Use NF4 quantization type for better precision\nuse_nested_quant = False # Activate nested quantization for more memory savings\n\n# TrainingArguments Configuration\noutput_dir = \"./gemma3-squad-finetuned\" # Directory to save the trained model adapters\nnum_train_epochs = 1 # Number of training epochs (adjust as needed)\nper_device_train_batch_size = 4 # Batch size per GPU\nper_device_eval_batch_size = 4 # Batch size for evaluation\ngradient_accumulation_steps = 2 # Number of steps to accumulate gradients before updating weights (effective batch size = 2 * 4 * 2 = 16)\ngradient_checkpointing = True # Use gradient checkpointing to save memory\noptim = \"paged_adamw_32bit\" # Use paged AdamW optimizer for memory efficiency\nlearning_rate = 2e-4 # Learning rate\nweight_decay = 0.001 # Weight decay for regularization\nmax_grad_norm = 0.3 # Gradient clipping threshold\nmax_steps = -1 # Maximum number of training steps (-1 for epoch-based training)\nwarmup_ratio = 0.03 # Ratio of steps for learning rate warmup\nlr_scheduler_type = \"constant\" # Learning rate scheduler type\nlogging_steps = 25 # Log training loss every 25 steps\neval_steps = 50 # Evaluate on validation set every 50 steps\nsave_steps = 50 # Save checkpoint every 50 steps\nevaluation_strategy = \"steps\" # Evaluate during training at `eval_steps`\nsave_strategy = \"steps\" # Save checkpoint during training at `save_steps`\nsave_total_limit = 2 # Keep only the last 2 checkpoints\nload_best_model_at_end = True # Load the best model found during training at the end\nreport_to = \"tensorboard\" # Log metrics to TensorBoard (useful in Kaggle)\n\n# SFTTrainer Configuration\nmax_seq_length = 512 # Maximum sequence length for tokenization\npacking = False # Whether to pack multiple sequences into one sample (set to False for QA)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Load Dataset and Preprocess\n\nWe'll load the SQuAD dataset and preprocess it into a format suitable for `SFTTrainer`. SFTTrainer expects a single text column containing the full input and output sequence. We will format each SQuAD example (context, question, answer) into a prompt that instructs the model to answer the question based on the context.\n\n**Formatting Strategy:**\n\nWe'll use a simple template:\n\n```\n<start_of_turn>user\nContext: [context]\nQuestion: [question]\nAnswer:<end_of_turn>\n<start_of_turn>model\n[answer]<end_of_turn>\n```\n\nThis follows the instruction-following format Gemma expects.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load and Preprocess Dataset ---\nfrom datasets import load_dataset\n\n# Load SQuAD dataset\ndataset = load_dataset(dataset_name, split='train') # Load the training split\n\n# SQuAD answers are structured. We need the first answer text.\ndef format_squad_example(example):\n    # Ensure 'answers' field exists and has 'text'\n    if 'answers' in example and 'text' in example['answers'] and len(example['answers']['text']) > 0:\n        context = example['context']\n        question = example['question']\n        answer = example['answers']['text'][0] # Take the first answer\n\n        # Create the formatted text string using the model's chat template structure\n        # Note: We are using a simplified template here. For best results,\n        # using the tokenizer's apply_chat_template might be preferable if available\n        # and correctly configured for the task.\n        formatted_text = f\"<start_of_turn>user\\nContext: {context}\\nQuestion: {question}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n{answer}<end_of_turn>\"\n        return {\"text\": formatted_text}\n    else:\n        # Handle cases where 'answers' might be missing or empty\n        # Returning None or an empty dict signals to filter this example out\n        return None\n\n\n# Apply the formatting function\n# We use map with batched=False as the logic is per-example.\n# remove_columns keeps only the new 'text' column SFTTrainer needs.\n# We also filter out examples that couldn't be formatted (returned None).\nformatted_dataset = dataset.map(format_squad_example, remove_columns=list(dataset.features))\nformatted_dataset = formatted_dataset.filter(lambda example: example['text'] is not None)\n\n\n# Split the dataset into training and validation sets\n# SQuAD doesn't have a predefined validation split in the same format,\n# so we create one from the training data.\ntrain_test_split = formatted_dataset.train_test_split(test_size=0.1) # 10% for validation\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(eval_dataset)}\")\nprint(\"\\nSample formatted example:\")\nprint(train_dataset[0]['text'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Load Model and Tokenizer\n\nNow, we load the Gemma 3 4B IT model and its tokenizer. We apply the 4-bit quantization configuration during model loading using `BitsAndBytesConfig`.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Model and Tokenizer ---\n\n# Load BitsAndBytes configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\n# device_map=\"auto\" will automatically distribute the model across available GPUs\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Automatically distribute model across GPUs\n    trust_remote_code=True # Gemma 3 might require this\n)\nmodel.config.use_cache = False # Disable cache for training\nmodel.config.pretraining_tp = 1 # Set tensor parallelism degree (1 for no TP)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n# Set padding token and side. For Gemma, the EOS token is often used as the PAD token.\n# Check tokenizer config or documentation if unsure. Let's assume EOS for now.\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # \"right\" is common for causal models, ensures labels are not padded","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Configure PEFT (LoRA)\n\nWe integrate PEFT into the loaded model. `prepare_model_for_kbit_training` prepares the quantized model for PEFT, and `get_peft_model` applies the LoRA configuration.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure PEFT ---\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules=lora_target_modules,\n    bias=\"none\", # Typically set to 'none' for LoRA\n    task_type=\"CAUSAL_LM\",\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Configure Training Arguments\n\nWe define the training parameters using `transformers.TrainingArguments`. This includes settings for batch size, learning rate, number of epochs, saving frequency, evaluation frequency, and more.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure Training Arguments ---\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=False, # fp16 not used with 4-bit, bf16 determined by compute_dtype\n    bf16=True if bnb_4bit_compute_dtype == 'bfloat16' and torch.cuda.is_bf16_supported() else False, # Use bf16 if supported for compute\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True, # Group sequences of similar length for efficiency\n    lr_scheduler_type=lr_scheduler_type,\n    logging_steps=logging_steps,\n    evaluation_strategy=evaluation_strategy,\n    eval_steps=eval_steps,\n    save_strategy=save_strategy,\n    save_steps=save_steps,\n    save_total_limit=save_total_limit,\n    load_best_model_at_end=load_best_model_at_end,\n    metric_for_best_model=\"eval_loss\", # Use eval loss to determine the best model\n    greater_is_better=False, # Lower eval loss is better\n    report_to=report_to,\n    push_to_hub=False, # Set to True if you want to push the adapter to Hub\n    gradient_checkpointing=gradient_checkpointing,\n    # ddp_find_unused_parameters=False, # Might be needed in some multi-GPU setups\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Initialize SFTTrainer and Start Training\n\nWe create an instance of `SFTTrainer`, passing the model, datasets, PEFT config, tokenizer, training arguments, and other relevant parameters. Then, we start the training process. The trainer will automatically handle the training loop, evaluation, logging (including training and validation loss), and saving checkpoints.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Initialize Trainer and Train ---\n\n# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\", # The column containing our formatted text\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Start training\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training finished.\")\n\n# Save the final adapter model\nfinal_adapter_dir = os.path.join(output_dir, \"final_adapter\")\ntrainer.model.save_pretrained(final_adapter_dir)\nprint(f\"Final PEFT adapter model saved to {final_adapter_dir}\")\n\n# --- Optional: Clean up memory ---\n# del model\n# del trainer\n# torch.cuda.empty_cache()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Evaluation and Inference (Optional)\n\nAfter training, the `trainer` object holds the training history, including training and validation loss, which can be accessed via `trainer.state.log_history`. TensorBoard logs will also be available in the `output_dir`.\n\nYou can also load the trained adapter model and perform inference.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Trained Model and Inference (Example) ---\nfrom peft import PeftModel\nimport torch\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- Reload Base Model and Tokenizer with Quantization ---\n# Ensure you reload the model in the same way you trained it (with quantization)\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# --- Load the PEFT Adapter ---\n# Use the path where the best model adapter was saved by the trainer\n# If load_best_model_at_end=True, the trainer saves it automatically.\n# Check the output_dir for checkpoint folders. Let's assume the final one is best for this example.\nadapter_path = final_adapter_dir # Or path to the best checkpoint, e.g., f\"{output_dir}/checkpoint-XXX\"\n\n# Load the LoRA adapter onto the base model\nmodel_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\nmodel_with_adapter = model_with_adapter.eval() # Set to evaluation mode\n\nprint(\"Loaded base model and adapter for inference.\")\n\n# --- Create Inference Pipeline ---\npipe = pipeline(\n    task=\"text-generation\",\n    model=model_with_adapter,\n    tokenizer=tokenizer,\n    max_new_tokens=50, # Limit the number of generated tokens (answer length)\n    # temperature=0.7, # Adjust creativity\n    # top_p=0.9,       # Use nucleus sampling\n    # repetition_penalty=1.1 # Penalize repetition\n)\n\n# --- Example Inference ---\n# Take an example from the original SQuAD validation set (or create one)\n# Note: Use the *original* SQuAD format here, not the training format.\n# The model expects the prompt format we used during fine-tuning.\n\n# Example from SQuAD dev set (you might need to load it separately)\ncontext_example = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ('Norman' comes from 'Norseman') raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\"\nquestion_example = \"Who were the Normans descended from?\"\n\n# Format the prompt like in training\nprompt = f\"<start_of_turn>user\\nContext: {context_example}\\nQuestion: {question_example}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n\"\n\n# Generate the answer\nresult = pipe(prompt)\n\nprint(\"\\n--- Inference Example ---\")\nprint(f\"Context: {context_example}\")\nprint(f\"Question: {question_example}\")\nprint(\"\\nGenerated Answer (following prompt):\")\n# The output includes the prompt, so we display the full generated text.\n# You might want to parse out just the answer part based on the <end_of_turn> token.\nprint(result[0]['generated_text'])\n\n# Parse the generated answer (simple approach)\ngenerated_full_text = result[0]['generated_text']\nanswer_start_tag = \"<start_of_turn>model\\n\"\nanswer_end_tag = \"<end_of_turn>\"\n\nstart_index = generated_full_text.find(answer_start_tag)\nif start_index != -1:\n    start_index += len(answer_start_tag)\n    end_index = generated_full_text.find(answer_end_tag, start_index)\n    if end_index != -1:\n        parsed_answer = generated_full_text[start_index:end_index].strip()\n        print(f\"\\nParsed Answer: {parsed_answer}\")\n    else:\n        # If end token not found, maybe take text after start token\n        parsed_answer = generated_full_text[start_index:].strip()\n        print(f\"\\nParsed Answer (end token not found): {parsed_answer}\")\nelse:\n    print(\"\\nCould not parse the answer using the expected format.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Conclusion\n\nThis notebook provided a comprehensive walkthrough for fine-tuning the Gemma 3 4B IT model on the SQuAD dataset using PEFT LoRA and 4-bit quantization within a Kaggle environment.\n\n**Key Takeaways:**\n\n* We successfully configured and ran a fine-tuning job using `SFTTrainer`.\n* PEFT LoRA allowed training with significantly fewer parameters.\n* 4-bit quantization drastically reduced memory requirements, making it feasible on T4 GPUs.\n* The training process tracked both training and validation loss, allowing monitoring of overfitting and model performance.\n* The final adapter model can be saved and used for inference on question-answering tasks formatted similarly to the training data.\n\n**Next Steps:**\n\n* Experiment with hyperparameters (learning rate, batch size, LoRA rank/alpha, number of epochs).\n* Try different prompt formatting strategies.\n* Evaluate the fine-tuned model using standard QA metrics (F1, Exact Match) on the SQuAD development set (requires custom evaluation logic).\n* Push the final adapter model to the Hugging Face Hub for easy sharing and reuse.","metadata":{}}]}