{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning Gemma 3 4B IT on SQuAD using PEFT LoRA on Kaggle\n\nThis notebook demonstrates how to fine-tune the `google/gemma-3-4b-it` model on the Stanford Question Answering Dataset (SQuAD) using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and 4-bit quantization. We'll leverage the `transformers`, `peft`, `trl`, `accelerate`, `bitsandbytes`, and `datasets` libraries within a Kaggle environment equipped with 2 T4 GPUs.\n\n**Goal:** Adapt the Gemma 3 instruction-tuned model to better handle question-answering tasks based on provided context, while optimizing resource usage.\n\n**Key Techniques:**\n\n1.  **Gemma 3 4B IT:** Utilizing Google's latest instruction-tuned model.\n2.  **SQuAD Dataset:** Training on a standard question-answering benchmark.\n3.  **PEFT & LoRA:** Significantly reducing the number of trainable parameters for efficient fine-tuning.\n4.  **4-bit Quantization:** Loading the base model in 4-bit precision to conserve GPU memory.\n5.  **SFTTrainer:** Using the `trl` library's trainer designed for supervised fine-tuning tasks.\n6.  **Kaggle Environment:** Utilizing the free T4 GPU resources.\n7.  **Loss Tracking:** Monitoring both training and validation loss during the process.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Kaggle Environment\n\n* **Enable GPUs:** Make sure to enable the GPU accelerator in your Kaggle notebook settings. Go to \"Settings\" -> \"Accelerator\" and select \"GPU T4 x2\".\n* **Hugging Face Token (Optional but Recommended):** While Gemma models might be accessible without login now, it's good practice to use a Hugging Face token, especially for potentially gated models or private use.\n    * Go to your Huggle Face account settings -> Access Tokens -> New token.\n    * In your Kaggle notebook, go to \"Add-ons\" -> \"Secrets\" and add your Hugging Face token with the name `HF_TOKEN`. The code below will attempt to log in using this secret.","metadata":{}},{"cell_type":"markdown","source":"## 2. Install Libraries\n\nWe need to install the necessary libraries from Hugging Face and other dependencies.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Install Libraries ---\n!pip install -q -U transformers datasets accelerate peft trl bitsandbytes huggingface_hub packaging ninja\n\n# The ninja package is often needed as a build dependency for bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:15:04.390938Z","iopub.execute_input":"2025-04-25T03:15:04.391240Z","iopub.status.idle":"2025-04-25T03:16:52.922349Z","shell.execute_reply.started":"2025-04-25T03:15:04.391218Z","shell.execute_reply":"2025-04-25T03:16:52.921266Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nlangchain-core 0.3.35 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip freeze | grep transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:16:52.923960Z","iopub.execute_input":"2025-04-25T03:16:52.924226Z","iopub.status.idle":"2025-04-25T03:16:54.856509Z","shell.execute_reply.started":"2025-04-25T03:16:52.924201Z","shell.execute_reply":"2025-04-25T03:16:54.855587Z"}},"outputs":[{"name":"stdout","text":"sentence-transformers==3.4.1\ntransformers==4.51.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Import Libraries & Login","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Imports and Login ---\nimport os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom trl import SFTTrainer, SFTConfig\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient # To access HF_TOKEN if set\n\n# --- Configuration ---\n# Attempt to log in to Hugging Face Hub\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN_EG\")\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face Hub.\")\nexcept Exception as e:\n    print(f\"Could not log in to Hugging Face Hub. Using public access. Error: {e}\")\n\n# Model ID\nmodel_id = \"google/gemma-3-4b-it\"\n\n# Dataset ID (Using Hugging Face's dataset identifier for SQuAD)\ndataset_name = \"squad\"\n\n# PEFT/LoRA Configuration\nlora_r = 16 # Rank of the LoRA matrices\nlora_alpha = 32 # Alpha parameter for LoRA scaling\nlora_dropout = 0.05 # Dropout probability for LoRA layers\n# Target modules can vary based on the model architecture.\n# Common targets for Gemma-like models include query, key, value, and output projections.\n# Inspecting model.named_modules() can help identify them.\n# Let's target common projection layers.\nlora_target_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n    \"down_proj\",\n]\n\n# BitsAndBytes Configuration (for 4-bit quantization)\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\" # Use bfloat16 for faster computation\nbnb_4bit_quant_type = \"nf4\" # Use NF4 quantization type for better precision\nuse_nested_quant = False # Activate nested quantization for more memory savings\n\n# TrainingArguments Configuration\noutput_dir = \"./gemma3-squad-finetuned\" # Directory to save the trained model adapters\nnum_train_epochs = 1 # Number of training epochs (adjust as needed)\nper_device_train_batch_size = 4 # Batch size per GPU\nper_device_eval_batch_size = 4 # Batch size for evaluation\ngradient_accumulation_steps = 2 # Number of steps to accumulate gradients before updating weights (effective batch size = 2 * 4 * 2 = 16)\ngradient_checkpointing = True # Use gradient checkpointing to save memory\noptim = \"paged_adamw_32bit\" # Use paged AdamW optimizer for memory efficiency\nlearning_rate = 2e-4 # Learning rate\nweight_decay = 0.001 # Weight decay for regularization\nmax_grad_norm = 0.3 # Gradient clipping threshold\nmax_steps = -1 # Maximum number of training steps (-1 for epoch-based training)\nwarmup_ratio = 0.03 # Ratio of steps for learning rate warmup\nlr_scheduler_type = \"constant\" # Learning rate scheduler type\nlogging_steps = 25 # Log training loss every 25 steps\neval_steps = 50 # Evaluate on validation set every 50 steps\nsave_steps = 50 # Save checkpoint every 50 steps\nevaluation_strategy = \"steps\" # Evaluate during training at `eval_steps`\nsave_strategy = \"steps\" # Save checkpoint during training at `save_steps`\nsave_total_limit = 2 # Keep only the last 2 checkpoints\nload_best_model_at_end = True # Load the best model found during training at the end\nreport_to = \"tensorboard\" # Log metrics to TensorBoard (useful in Kaggle)\n\n# SFTTrainer Configuration\nmax_seq_length = 512 # Maximum sequence length for tokenization\npacking = False # Whether to pack multiple sequences into one sample (set to False for QA)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:16:54.857703Z","iopub.execute_input":"2025-04-25T03:16:54.858011Z","iopub.status.idle":"2025-04-25T03:17:35.174903Z","shell.execute_reply.started":"2025-04-25T03:16:54.857985Z","shell.execute_reply":"2025-04-25T03:17:35.174152Z"}},"outputs":[{"name":"stderr","text":"2025-04-25 03:17:13.395726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745551033.765899      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745551033.867088      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged into Hugging Face Hub.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Load Dataset and Preprocess\n\nWe'll load the SQuAD dataset and preprocess it into a format suitable for `SFTTrainer`. SFTTrainer expects a single text column containing the full input and output sequence. We will format each SQuAD example (context, question, answer) into a prompt that instructs the model to answer the question based on the context.\n\n**Formatting Strategy:**\n\nWe'll use a simple template:\n\n```\n<start_of_turn>user\nContext: [context]\nQuestion: [question]\nAnswer:<end_of_turn>\n<start_of_turn>model\n[answer]<end_of_turn>\n```\n\nThis follows the instruction-following format Gemma expects.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load and Preprocess Dataset ---\nfrom datasets import load_dataset\n\n# Load SQuAD dataset\ndataset =  load_dataset(dataset_name, split='train') # Load the training split\n\n# SQuAD answers are structured. We need the first answer text.\ndef format_squad_example(example):\n    # Ensure 'answers' field exists and has 'text'\n    if 'answers' in example and 'text' in example['answers'] and len(example['answers']['text']) > 0:\n        context = example['context']\n        question = example['question']\n        answer = example['answers']['text'][0] # Take the first answer\n\n        # Create the formatted text string using the model's chat template structure\n        # Note: We are using a simplified template here. For best results,\n        # using the tokenizer's apply_chat_template might be preferable if available\n        # and correctly configured for the task.\n        formatted_text = f\"<start_of_turn>user\\nContext: {context}\\nQuestion: {question}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n{answer}<end_of_turn>\"\n        return {\"text\": formatted_text}\n    else:\n        # Handle cases where 'answers' might be missing or empty\n        # Returning None or an empty dict signals to filter this example out\n        return None\n\n\n# Apply the formatting function\n# We use map with batched=False as the logic is per-example.\n# remove_columns keeps only the new 'text' column SFTTrainer needs.\n# We also filter out examples that couldn't be formatted (returned None).\nformatted_dataset = dataset.map(format_squad_example, remove_columns=list(dataset.features))\nformatted_dataset = formatted_dataset.filter(lambda example: example['text'] is not None)\n\n# Take subset for faster experimentation (avoid OutOfMemoryError)\nformatted_dataset = formatted_dataset.select(range(5000))\n\n\n# Split the dataset into training and validation sets\n# SQuAD doesn't have a predefined validation split in the same format,\n# so we create one from the training data.\ntrain_test_split = formatted_dataset.train_test_split(test_size=0.1) # 10% for validation\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(eval_dataset)}\")\nprint(\"\\nSample formatted example:\")\nprint(train_dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:17:35.176374Z","iopub.execute_input":"2025-04-25T03:17:35.176598Z","iopub.status.idle":"2025-04-25T03:17:47.211382Z","shell.execute_reply.started":"2025-04-25T03:17:35.176580Z","shell.execute_reply":"2025-04-25T03:17:47.210797Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa773714d3ee48a3b260136efb7810f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"983e3b066408499ebbd266a65ff8c044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b145b1e54a47d6a0728ff34d36e9b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922afcc490e04b22a251310b066d8451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd5abf9e1c24eaaa4576a46bc1188b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19156ecf902f4d02a3c456e433a07049"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed20ac6d0894ba7a7f8ab5579085bc4"}},"metadata":{}},{"name":"stdout","text":"Training dataset size: 4500\nValidation dataset size: 500\n\nSample formatted example:\n<start_of_turn>user\nContext: Beyoncé's second solo album B'Day was released on September 5, 2006, in the US, to coincide with her twenty-fifth birthday. It sold 541,000 copies in its first week and debuted atop the Billboard 200, becoming Beyoncé's second consecutive number-one album in the United States. The album's lead single \"Déjà Vu\", featuring Jay Z, reached the top five on the Billboard Hot 100 chart. The second international single \"Irreplaceable\" was a commercial success worldwide, reaching number one in Australia, Hungary, Ireland, New Zealand and the United States. B'Day also produced three other singles; \"Ring the Alarm\", \"Get Me Bodied\", and \"Green Light\" (released in the United Kingdom only).\nQuestion: What artist did Beyonce duet with in the single, \"Deja Vu''?\nAnswer:<end_of_turn>\n<start_of_turn>model\nJay Z<end_of_turn>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. Load Model and Tokenizer\n\nNow, we load the Gemma 3 4B IT model and its tokenizer. We apply the 4-bit quantization configuration during model loading using `BitsAndBytesConfig`.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Model and Tokenizer ---\n\n# Load BitsAndBytes configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\n# device_map=\"auto\" will automatically distribute the model across available GPUs\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Automatically distribute model across GPUs ------------- # KIV cuda mapping, might cause Tensor error at validation\n    trust_remote_code=True, # Gemma 3 might require this\n    attn_implementation='eager' # 'sdpa' not compatible\n)\nmodel.config.use_cache = False # Disable cache for training\nmodel.config.pretraining_tp = 1 # Set tensor parallelism degree (1 for no TP)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n# Set padding token and side. For Gemma, the EOS token is often used as the PAD token.\n# Check tokenizer config or documentation if unsure. Let's assume EOS for now.\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # \"right\" is common for causal models, ensures labels are not padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:21:05.194283Z","iopub.execute_input":"2025-04-25T03:21:05.195018Z","iopub.status.idle":"2025-04-25T03:21:18.530835Z","shell.execute_reply.started":"2025-04-25T03:21:05.194978Z","shell.execute_reply":"2025-04-25T03:21:18.529987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb430a2424b449ad8ea865929ff3412e"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## 6. Configure PEFT (LoRA)\n\nWe integrate PEFT into the loaded model. `prepare_model_for_kbit_training` prepares the quantized model for PEFT, and `get_peft_model` applies the LoRA configuration.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure PEFT ---\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules=lora_target_modules,\n    bias=\"none\", # Typically set to 'none' for LoRA\n    task_type=\"CAUSAL_LM\",\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:21:18.532480Z","iopub.execute_input":"2025-04-25T03:21:18.532767Z","iopub.status.idle":"2025-04-25T03:21:19.340385Z","shell.execute_reply.started":"2025-04-25T03:21:18.532747Z","shell.execute_reply":"2025-04-25T03:21:19.339722Z"}},"outputs":[{"name":"stdout","text":"trainable params: 32,788,480 || all params: 4,332,867,952 || trainable%: 0.7567\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 7. Configure Training Arguments\n\nWe define the training parameters using `transformers.TrainingArguments`. This includes settings for batch size, learning rate, number of epochs, saving frequency, evaluation frequency, and more.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Configure Training Arguments ---\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=False, # fp16 not used with 4-bit, bf16 determined by compute_dtype\n    bf16=True if bnb_4bit_compute_dtype == 'bfloat16' and torch.cuda.is_bf16_supported() else False, # Use bf16 if supported for compute\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True, # Group sequences of similar length for efficiency\n    lr_scheduler_type=lr_scheduler_type,\n    logging_steps=logging_steps,\n    eval_strategy=evaluation_strategy,\n    eval_steps=eval_steps,\n    save_strategy=save_strategy,\n    save_steps=save_steps,\n    save_total_limit=save_total_limit,\n    load_best_model_at_end=load_best_model_at_end,\n    metric_for_best_model=\"eval_loss\", # Use eval loss to determine the best model\n    greater_is_better=False, # Lower eval loss is better\n    report_to=report_to,\n    push_to_hub=False, # Set to True if you want to push the adapter to Hub\n    gradient_checkpointing=gradient_checkpointing,\n    # ddp_find_unused_parameters=False, # Might be needed in some multi-GPU setups\n    dataset_text_field=\"text\", # The column containing our formatted text ###\n    max_seq_length=max_seq_length,  ###\n    packing=packing  ###\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:21:19.341117Z","iopub.execute_input":"2025-04-25T03:21:19.341332Z","iopub.status.idle":"2025-04-25T03:21:19.384202Z","shell.execute_reply.started":"2025-04-25T03:21:19.341311Z","shell.execute_reply":"2025-04-25T03:21:19.383180Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 8. Initialize SFTTrainer and Start Training\n\nWe create an instance of `SFTTrainer`, passing the model, datasets, PEFT config, tokenizer, training arguments, and other relevant parameters. Then, we start the training process. The trainer will automatically handle the training loop, evaluation, logging (including training and validation loss), and saving checkpoints.","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Initialize Trainer and Train ---\n\n# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    # dataset_text_field=\"text\", # The column containing our formatted text\n    # max_seq_length=max_seq_length,\n    processing_class=tokenizer,\n    args=training_arguments,\n    # packing=packing,\n)\n\n# Start training\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training finished.\")\n\n# Save the final adapter model\nfinal_adapter_dir = os.path.join(output_dir, \"final_adapter\")\ntrainer.model.save_pretrained(final_adapter_dir)\nprint(f\"Final PEFT adapter model saved to {final_adapter_dir}\")\n\n# --- Optional: Clean up memory ---\n# del model\n# del trainer\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:21:19.386275Z","iopub.execute_input":"2025-04-25T03:21:19.387032Z","iopub.status.idle":"2025-04-25T06:13:19.571276Z","shell.execute_reply.started":"2025-04-25T03:21:19.387002Z","shell.execute_reply":"2025-04-25T06:13:19.570623Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7415bce854624a318db60804802ca691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260401b955444932ab75f5362b872946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45b7d28569845749a4265df161c2780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d1c0b1bed1447cfa463eba6f58ddcd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d7a27a1f684308869d6f4134a5ccae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32127da2e3634e0f9ab6d465338271fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e84db9d3bd6411bb4486358896ec014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c806aa2a6dd84c30982c8d238f0e5a0d"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [562/562 2:51:22, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>3.892300</td>\n      <td>1.949782</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.457700</td>\n      <td>1.713886</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.136000</td>\n      <td>1.560015</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.976400</td>\n      <td>1.402090</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.626400</td>\n      <td>1.240367</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.180300</td>\n      <td>1.111201</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.020800</td>\n      <td>0.976760</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.710300</td>\n      <td>0.863557</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.567500</td>\n      <td>0.765786</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.373100</td>\n      <td>0.694445</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.141100</td>\n      <td>0.601106</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training finished.\nFinal PEFT adapter model saved to ./gemma3-squad-finetuned/final_adapter\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 9. Evaluation and Inference (Optional)\n\nAfter training, the `trainer` object holds the training history, including training and validation loss, which can be accessed via `trainer.state.log_history`. TensorBoard logs will also be available in the `output_dir`.\n\nYou can also load the trained adapter model and perform inference.\n","metadata":{}},{"cell_type":"code","source":"# --- Code Cell: Load Trained Model and Inference (Example) ---\nfrom peft import PeftModel\nimport torch\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- Reload Base Model and Tokenizer with Quantization ---\n# Ensure you reload the model in the same way you trained it (with quantization)\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# --- Load the PEFT Adapter ---\n# Use the path where the best model adapter was saved by the trainer\n# If load_best_model_at_end=True, the trainer saves it automatically.\n# Check the output_dir for checkpoint folders. Let's assume the final one is best for this example.\nadapter_path = \"/kaggle/working/gemma3-squad-finetuned/final_adapter\" # Or path to the best checkpoint, e.g., f\"{output_dir}/checkpoint-XXX\"\n\n# Load the LoRA adapter onto the base model\nmodel_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\nmodel_with_adapter = model_with_adapter.eval() # Set to evaluation mode\n\nprint(\"Loaded base model and adapter for inference.\")\n\n# --- Create Inference Pipeline ---\npipe = pipeline(\n    task=\"text-generation\",\n    model=model_with_adapter,\n    tokenizer=tokenizer,\n    max_new_tokens=50, # Limit the number of generated tokens (answer length)\n    # temperature=0.7, # Adjust creativity\n    # top_p=0.9,       # Use nucleus sampling\n    # repetition_penalty=1.1 # Penalize repetition\n)\n\n# --- Example Inference ---\n# Take an example from the original SQuAD validation set (or create one)\n# Note: Use the *original* SQuAD format here, not the training format.\n# The model expects the prompt format we used during fine-tuning.\n\n# Example from SQuAD dev set (you might need to load it separately)\ncontext_example = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ('Norman' comes from 'Norseman') raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\"\nquestion_example = \"Who were the Normans descended from?\"\n\n# Format the prompt like in training\nprompt = f\"<start_of_turn>user\\nContext: {context_example}\\nQuestion: {question_example}\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\n\"\n\n# Generate the answer\nresult = pipe(prompt)\n\nprint(\"\\n--- Inference Example ---\")\nprint(f\"Context: {context_example}\")\nprint(f\"Question: {question_example}\")\nprint(\"\\nGenerated Answer (following prompt):\")\n# The output includes the prompt, so we display the full generated text.\n# You might want to parse out just the answer part based on the <end_of_turn> token.\nprint(result[0]['generated_text'])\n\n# Parse the generated answer (simple approach)\ngenerated_full_text = result[0]['generated_text']\nanswer_start_tag = \"<start_of_turn>model\\n\"\nanswer_end_tag = \"<end_of_turn>\"\n\nstart_index = generated_full_text.find(answer_start_tag)\nif start_index != -1:\n    start_index += len(answer_start_tag)\n    end_index = generated_full_text.find(answer_end_tag, start_index)\n    if end_index != -1:\n        parsed_answer = generated_full_text[start_index:end_index].strip()\n        print(f\"\\nParsed Answer: {parsed_answer}\")\n    else:\n        # If end token not found, maybe take text after start token\n        parsed_answer = generated_full_text[start_index:].strip()\n        print(f\"\\nParsed Answer (end token not found): {parsed_answer}\")\nelse:\n    print(\"\\nCould not parse the answer using the expected format.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:14:19.711330Z","iopub.execute_input":"2025-04-25T06:14:19.711980Z","iopub.status.idle":"2025-04-25T06:14:20.275420Z","shell.execute_reply.started":"2025-04-25T06:14:19.711956Z","shell.execute_reply":"2025-04-25T06:14:20.274224Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2258236846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "],"ename":"ValueError","evalue":"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## 10. Conclusion\n\nThis notebook provided a comprehensive walkthrough for fine-tuning the Gemma 3 4B IT model on the SQuAD dataset using PEFT LoRA and 4-bit quantization within a Kaggle environment.\n\n**Key Takeaways:**\n\n* We successfully configured and ran a fine-tuning job using `SFTTrainer`.\n* PEFT LoRA allowed training with significantly fewer parameters.\n* 4-bit quantization drastically reduced memory requirements, making it feasible on T4 GPUs.\n* The training process tracked both training and validation loss, allowing monitoring of overfitting and model performance.\n* The final adapter model can be saved and used for inference on question-answering tasks formatted similarly to the training data.\n\n**Next Steps:**\n\n* Experiment with hyperparameters (learning rate, batch size, LoRA rank/alpha, number of epochs).\n* Try different prompt formatting strategies.\n* Evaluate the fine-tuned model using standard QA metrics (F1, Exact Match) on the SQuAD development set (requires custom evaluation logic).\n* Push the final adapter model to the Hugging Face Hub for easy sharing and reuse.","metadata":{}}]}